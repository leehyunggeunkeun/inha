{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO98TyrUeSnMTazV8m2Xqre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehyunggeunkeun/pytorch-study/blob/master/DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCzUM1Hh_2y7",
        "colab_type": "text"
      },
      "source": [
        "MNIST 데이터셋으로 softmax classfication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwHwzZmhuTh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#edwith 부스트코스 파이토치 강의를 보면서 필사하고 느낀점 및 방법들을 까먹지 않게 정리함...\n",
        "#출처 edwith 부스트 코스 pytorch 강의 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFDZZWhP_-xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#필요한 라이브러리 불러오기\n",
        "\n",
        "#기본\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#기존 데이터셋 불러오기\n",
        "import torchvision.datasets as dsets         \n",
        "import torchvision.transforms as transforms  #다양한 이미지 변환\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "conv.torch.nn.Conv2d()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13mzi89-AhJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gpu 사용하기\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "random.seed(111)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFpAipMIA4ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters\n",
        "\n",
        "lr = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBzY6cjIBDSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train = dsets.MNIST(root='MNIST_data/',train=True,transform=transforms.ToTensor(),download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',train=False,transform=transforms.ToTensor(),download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yqn6cJABri3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#데이터셋 불러오기\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18EYREIhB7d4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer를 한번 쌓아보자....\n",
        "\n",
        "linear1=torch.nn.Linear(784,256,bias=True)\n",
        "linear2=torch.nn.Linear(256,128,bias=True)\n",
        "linear3=torch.nn.Linear(128,64,bias=True)\n",
        "linear4=torch.nn.Linear(64,32,bias=True)    \n",
        "linear5=torch.nn.Linear(32,10,bias=True) \n",
        "\n",
        "relu=torch.nn.ReLU()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGufNjEQCEBF",
        "colab_type": "code",
        "outputId": "305ff2fe-0d4b-4369-b821-03b09dc1ac69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "#초기화\n",
        "torch.nn.init.normal_(linear1.weight)\n",
        "torch.nn.init.normal_(linear2.weight)\n",
        "torch.nn.init.normal_(linear3.weight)\n",
        "torch.nn.init.normal_(linear4.weight)\n",
        "torch.nn.init.normal_(linear5.weight) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.8760, -0.2428, -0.0590,  0.1154,  0.1392, -2.1023, -1.3665,  1.6940,\n",
              "         -0.3535, -0.4622, -0.9144, -2.4040, -1.0246,  1.2613, -0.8845, -0.5964,\n",
              "          0.8419, -0.2984, -0.2444, -0.7330,  0.6643,  0.3606,  0.0785, -0.0251,\n",
              "          0.1463,  0.6727, -0.9334,  0.0913, -0.2854,  1.2619,  0.0364,  1.2454],\n",
              "        [-0.4429,  0.1839,  1.3681, -0.0368, -0.1383,  1.3645,  0.1795,  0.3320,\n",
              "         -0.7304,  1.1410,  1.8497,  1.3517, -3.2752,  2.0577,  0.7026,  0.6041,\n",
              "          0.2571,  1.9346, -0.5456, -1.4699,  0.6691, -3.0588, -1.0069, -0.1531,\n",
              "         -2.0676,  1.0222,  0.0523, -0.5590,  0.2663,  0.7497, -0.9099,  1.4027],\n",
              "        [ 0.3852,  2.2101, -1.5015, -0.6829,  0.3430, -0.5899, -0.5412, -1.3859,\n",
              "          1.5074, -0.9125, -0.4844, -0.5480, -0.4807, -0.5619,  0.5002,  1.0479,\n",
              "          1.5341, -0.9516,  0.0034,  0.6849,  0.0748, -0.2988, -0.8951,  0.0170,\n",
              "          0.1203, -0.2802, -1.2814,  1.2865,  0.2399,  0.3876,  1.4302, -0.7666],\n",
              "        [-1.8724, -0.2514, -0.9778, -0.0463, -0.2349,  0.6859,  0.2419,  1.8770,\n",
              "          0.3279,  0.9151,  0.6328,  0.1364,  0.0112, -1.0298,  0.0298,  0.2789,\n",
              "          0.1822,  0.3317, -0.6880, -0.2385,  0.2597,  0.6004,  0.4157,  0.4556,\n",
              "         -0.2989, -0.6979,  1.0488, -0.7182,  0.6565, -0.6063, -2.1994, -0.3400],\n",
              "        [ 1.5934,  0.4671, -0.6022,  0.4055, -1.0027,  0.2390,  0.5056,  0.5204,\n",
              "          0.1854, -1.1271,  0.6965,  1.1637,  1.3512,  0.2747,  0.0983,  0.5796,\n",
              "         -0.6514, -0.8025, -0.9902, -1.4581,  0.5787,  0.7435,  0.1690,  0.8503,\n",
              "          0.7681,  0.6488,  1.1507, -0.4447,  1.9859, -1.2649,  1.4433,  1.5439],\n",
              "        [-0.2760, -0.7097,  1.1859,  0.8677,  0.0380, -1.3708,  0.7039,  0.4456,\n",
              "         -0.4027, -0.3856, -0.6638, -0.1493, -1.0177,  1.5190, -0.6962,  0.5163,\n",
              "         -1.0773, -0.9040, -0.0802,  0.1042, -1.3553,  0.1056, -0.7934, -1.3633,\n",
              "         -0.5651, -0.4734,  0.3707, -1.3995, -0.5507, -0.8454, -0.3222,  0.0902],\n",
              "        [-0.1954, -0.3645, -0.1377,  1.2979,  0.5955,  1.5258, -0.6381,  0.0410,\n",
              "          0.9770,  1.4061,  0.0794,  0.3666, -0.9110,  1.5838, -1.5427,  1.0272,\n",
              "         -0.3731,  0.2619,  0.1821, -0.4752,  0.4237,  0.2654,  0.0284,  0.7251,\n",
              "          0.0255,  0.3605, -0.4610,  0.3641, -1.2300,  0.1145, -1.4955, -0.2016],\n",
              "        [ 1.2548,  0.5632, -0.2714,  2.0952,  0.9674,  0.8014,  0.2340, -0.7340,\n",
              "         -0.7106,  0.0911,  0.1824,  0.0625, -1.6248,  1.0412,  1.6765, -0.0241,\n",
              "         -0.1715, -0.4244, -0.0759,  0.6378,  0.0660,  0.5268,  1.1630,  0.7519,\n",
              "         -0.5462,  1.6258,  0.1170,  0.0164, -1.1678, -0.6859, -0.5075,  1.0143],\n",
              "        [ 1.7716, -0.2734, -1.4921, -0.5095,  1.5223, -1.0865,  0.1057,  1.4151,\n",
              "          1.6764, -1.2402, -0.3336,  0.3578,  0.3547, -0.3528,  0.8346,  0.2985,\n",
              "         -1.5384,  0.4605, -0.2662, -0.0789, -1.2371,  1.2775,  0.1596, -0.2021,\n",
              "          1.1615, -0.2041,  0.7810,  2.6347, -1.8458, -1.0159, -0.6744, -0.5237],\n",
              "        [ 0.8648, -1.3729,  0.1207, -1.1715, -0.8528,  0.2466,  1.1905, -0.2676,\n",
              "         -0.2561, -1.5826, -0.0891, -0.5608, -0.6481, -1.1053,  0.2668,  1.2709,\n",
              "         -0.1136,  0.8068,  1.1440, -0.0965,  0.4554,  1.1226,  0.9616,  0.3360,\n",
              "         -0.5901, -0.3998, -0.6851, -0.7589,  0.9744,  1.3738,  0.4867,  1.9154]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ8pDO3FFdKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.nn.Sequential(linear1, relu,linear2, relu,linear3,relu,linear4,relu,linear5).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNOpWUYLCSbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(device)     \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44fpEw-8DWxT",
        "colab_type": "code",
        "outputId": "9234a6e8-733a-48ab-b8ff-891ee98d129c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "total_batch=len(dataloader)\n",
        "\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost=0\n",
        "\n",
        "    for x,y in dataloader:\n",
        "        x=x.view(-1,784).to(device)\n",
        "        y=y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis=model(x)\n",
        "        cost = criterion(hypothesis, y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 1525.619140625\n",
            "Epoch: 0002 cost = 276.732330322\n",
            "Epoch: 0003 cost = 155.909927368\n",
            "Epoch: 0004 cost = 101.592491150\n",
            "Epoch: 0005 cost = 71.055374146\n",
            "Epoch: 0006 cost = 51.889701843\n",
            "Epoch: 0007 cost = 38.824405670\n",
            "Epoch: 0008 cost = 29.453872681\n",
            "Epoch: 0009 cost = 22.782083511\n",
            "Epoch: 0010 cost = 17.747697830\n",
            "Epoch: 0011 cost = 13.830949783\n",
            "Epoch: 0012 cost = 11.233580589\n",
            "Epoch: 0013 cost = 9.198373795\n",
            "Epoch: 0014 cost = 7.390275002\n",
            "Epoch: 0015 cost = 6.056800365\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52yjNTpfD9bX",
        "colab_type": "code",
        "outputId": "7e82e536-5ddd-407f-a8c8-ae58a6359f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    \n",
        "    x_test = mnist_test.data.view(-1, 784).float().to(device)\n",
        "    y_test = mnist_test.targets.to(device)\n",
        "\n",
        "    prediction = model(x_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    x_single_data = mnist_test.data[r:r + 1].view(-1, 784).float().to(device)\n",
        "    y_single_data = mnist_test.targets[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', y_single_data.item())\n",
        "    single_prediction = model(x_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9131999611854553\n",
            "Label:  9\n",
            "Prediction:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBLbIyP02MHp",
        "colab_type": "code",
        "outputId": "d355e6ea-7ca7-4858-f7c2-6bfaab953390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "df = load_iris()\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv',\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i4fbZKnR-3p",
        "colab_type": "text"
      },
      "source": [
        "cs231n 강의에서도 딴거 말고 걍 Batch Normalization 하라 했음...\n",
        "Batch Normalization 에 대해 알아보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoNu1bXhSHya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lab 10 MNIST and softmax\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pylab as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW4AnrR-r7LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(1)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEoxfhSvr7P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 10\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNnOH7gtr7R6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hEevDiqr7UF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13nSxl1or7N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "bn1 = torch.nn.BatchNorm1d(32)\n",
        "bn2 = torch.nn.BatchNorm1d(32)\n",
        "\n",
        "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
        "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
        "nn_linear3 = torch.nn.Linear(32, 10, bias=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GGzVVussEBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "bn_model = torch.nn.Sequential(linear1, bn1, relu,\n",
        "                            linear2, bn2, relu,\n",
        "                            linear3).to(device)\n",
        "nn_model = torch.nn.Sequential(nn_linear1, relu,\n",
        "                               nn_linear2, relu,\n",
        "                               nn_linear3).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3hJzjrpsFex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n",
        "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdVP9QsFsGtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "dae6584f-b37a-4547-ba4e-99a909c574bb"
      },
      "source": [
        "\n",
        "# Save Losses and Accuracies every epoch\n",
        "# We are going to plot them later\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "train_total_batch = len(train_loader)\n",
        "test_total_batch = len(test_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    bn_model.train()  # set the model to train mode\n",
        "\n",
        "    for X, Y in train_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        bn_optimizer.zero_grad()\n",
        "        bn_prediction = bn_model(X)\n",
        "        bn_loss = criterion(bn_prediction, Y)\n",
        "        bn_loss.backward()\n",
        "        bn_optimizer.step()\n",
        "\n",
        "        nn_optimizer.zero_grad()\n",
        "        nn_prediction = nn_model(X)\n",
        "        nn_loss = criterion(nn_prediction, Y)\n",
        "        nn_loss.backward()\n",
        "        nn_optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bn_model.eval()     # set the model to evaluation mode\n",
        "\n",
        "        # Test the model using train sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(train_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch\n",
        "\n",
        "        # Save train losses/acc\n",
        "        train_losses.append([bn_loss, nn_loss])\n",
        "        train_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        # Test the model using test sets\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(test_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = bn_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            nn_prediction = nn_model(X)\n",
        "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
        "            nn_loss += criterion(nn_prediction, Y)\n",
        "            nn_acc += nn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\n",
        "\n",
        "        # Save valid losses/acc\n",
        "        valid_losses.append([bn_loss, nn_loss])\n",
        "        valid_accs.append([bn_acc, nn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
        "                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
        "        print()\n",
        "\n",
        "print('Learning finished')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.13505(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.17104(nn_acc:0.95)\n",
            "[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.14253(bn_acc:0.95) vs No Batchnorm Loss(Acc): nn_loss:0.19548(nn_acc:0.94)\n",
            "\n",
            "[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.10647(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.19554(nn_acc:0.94)\n",
            "[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.12260(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.22271(nn_acc:0.94)\n",
            "\n",
            "[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.09675(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.17482(nn_acc:0.95)\n",
            "[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.11440(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.19833(nn_acc:0.94)\n",
            "\n",
            "[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08105(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.17992(nn_acc:0.95)\n",
            "[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.10669(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.21152(nn_acc:0.94)\n",
            "\n",
            "[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06809(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.15283(nn_acc:0.96)\n",
            "[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.10304(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.20713(nn_acc:0.95)\n",
            "\n",
            "[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06950(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.14759(nn_acc:0.96)\n",
            "[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.10549(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18754(nn_acc:0.95)\n",
            "\n",
            "[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06137(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.13274(nn_acc:0.96)\n",
            "[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.09289(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18556(nn_acc:0.95)\n",
            "\n",
            "[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05521(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.15741(nn_acc:0.96)\n",
            "[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.09009(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.20443(nn_acc:0.95)\n",
            "\n",
            "[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.04948(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12258(nn_acc:0.97)\n",
            "[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.08718(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.16542(nn_acc:0.96)\n",
            "\n",
            "[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.04612(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.14519(nn_acc:0.96)\n",
            "[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.08525(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.19823(nn_acc:0.95)\n",
            "\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2E9IEWKsM2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}