{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_0601_자연어전처리.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrDO6JlO8vEMtJ4+xVJRWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehyunggeunkeun/pytorch-study/blob/master/pytorch_0601_%EC%9E%90%EC%97%B0%EC%96%B4%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDm8z1jxnakx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#자연어 전처리를 해보자\n",
        "#이걸 하면서 크롤링도 간단히 해보고 자연어 전처리의 방법등을 알 수 있으니 기본부터 입력하자"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQJZJrXWn-Xb",
        "colab_type": "text"
      },
      "source": [
        "네이버 영화 댓글 크롤링(클레멘타인)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o2ezpHFoAvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 크롤링 라이브러리 불러오기\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wI_y0HppBBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e46c4894-7a47-46d5-e0a3-e60eb7712c2c"
      },
      "source": [
        "# 개발자도구 들어가서 나오는 url을 입력함\n",
        "# 예전에 어디서 보기로 페이지가 바뀔때 url이 바뀌면 정적페이지고 그러면 거의 html방식으로 하고 url이 바뀌지않는 동적페이지면 json으로 한다고 했었는데\n",
        "# 페이지 1,2,3 클릭결과 url 바뀌는거 보고 html이겠거니 생각함....\n",
        "\n",
        "\n",
        "url='https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=37886&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page=1'\n",
        "response=requests.get(url)\n",
        "html= BeautifulSoup(response.content,'html.parser')\n",
        "html"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "<!DOCTYPE html>\n",
              "\n",
              "<html lang=\"ko\">\n",
              "<head>\n",
              "<meta charset=\"utf-8\"/>\n",
              "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
              "<title>네이버 영화</title>\n",
              "<link href=\"https://ssl.pstatic.net/static/m/movie/icons/naver_movie_favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/>\n",
              "<link href=\"/css/common.css?20200515103320\" rel=\"stylesheet\" type=\"text/css\">\n",
              "<link href=\"/css/movie_tablet.css?20200515103320\" rel=\"stylesheet\" type=\"text/css\"/>\n",
              "<link href=\"/css/movie_end.css?20200515103320\" rel=\"stylesheet\" type=\"text/css\"/>\n",
              "<script src=\"/js/deploy/movie.all.js?20200515103320\" type=\"text/javascript\"></script>\n",
              "</link></head>\n",
              "<body>\n",
              "<!-- content -->\n",
              "<input id=\"movieCode\" name=\"movieCode\" type=\"hidden\" value=\"37886\"/>\n",
              "<input id=\"onlyActualPointYn\" name=\"onlyActualPointYn\" type=\"hidden\" value=\"N\"/>\n",
              "<input id=\"includeSpoilerYn\" name=\"includeSpoilerYn\" type=\"hidden\" value=\"N\"/>\n",
              "<input id=\"order\" name=\"order\" type=\"hidden\" value=\"sympathyScore\"/>\n",
              "<input id=\"page\" name=\"page\" type=\"hidden\" value=\"1\"/>\n",
              "<div class=\"ifr_area basic_ifr\">\n",
              "<div class=\"input_netizen \">\n",
              "<!-- [D] 관람객 평점 작성 완료 -->\n",
              "<div class=\"ly_viewer\" id=\"actualPointWriteExecuteLayer\" style=\"display:none\">\n",
              "<h4>관람객 평점 작성 완료 안내</h4>\n",
              "<p>관람객 평점이 등록되었습니다.<br/><em>네이버페이 포인트 500원</em>이 적립되었습니다.<br/><em>7일 이후</em> 확인 가능합니다.</p>\n",
              "<p>(평점 삭제시, 적립된 포인트는 회수됩니다.)</p>\n",
              "<div class=\"btn\">\n",
              "<a class=\"ok\" href=\"#\" id=\"actualPointWriteExecuteLayerOkButton\">확인</a>\n",
              "<a class=\"close\" href=\"#\" id=\"actualPointWriteExecuteLayerCloseButton\" title=\"닫기\">관람객 평점 작성 완료 안내 레이어 닫기</a>\n",
              "</div>\n",
              "</div>\n",
              "<!-- //관람객 평점 작성 완료 -->\n",
              "<!-- [D] 관람객 평점 작성 완료2 -->\n",
              "<div class=\"ly_viewer\" id=\"pointWriteExecuteLayer\" style=\"display:none\">\n",
              "<h4>관람객 평점 작성 완료 안내</h4>\n",
              "<p class=\"msg1\">관람객 평점이 등록되었습니다.</p>\n",
              "<div class=\"btn\">\n",
              "<a class=\"ok\" href=\"#\" id=\"pointWriteExecuteLayerOkButton\">확인</a>\n",
              "<a class=\"close\" href=\"#\" id=\"pointWriteExecuteLayerCloseButton\" title=\"닫기\">관람객 평점 작성 완료 안내 레이어 닫기</a>\n",
              "</div>\n",
              "</div>\n",
              "<!-- //관람객 평점 작성 완료2 -->\n",
              "<div class=\"score_total\">\n",
              "<strong class=\"total\">관람객 평점 <em>21,711</em>건<button class=\"btn_review\" id=\"open-form-btn\">내 평점 등록</button></strong>\n",
              "</div>\n",
              "<div class=\"top_behavior\" id=\"orderCheckbox\">\n",
              "<ul class=\"sorting_list\">\n",
              "<li class=\"on\"><a href=\"#\" onclick=\"parent.clickcr(this, 'ara.bysym', '', '', event); dislplayOrder('sympathyScore');\">공감순</a></li>\n",
              "<li><a href=\"#\" onclick=\"parent.clickcr(this, 'ara.byrct', '', '', event); dislplayOrder('newest');\">최신순</a></li>\n",
              "<li><a href=\"#\" onclick=\"parent.clickcr(this, 'ara.high', '', '', event); dislplayOrder('highest');\">평점 높은 순</a></li>\n",
              "<li><a href=\"#\" onclick=\"parent.clickcr(this, 'ara.low', '', '', event); dislplayOrder('lowest');\">평점 낮은 순</a></li>\n",
              "</ul>\n",
              "<ul class=\"quarter_mode\">\n",
              "<li>\n",
              "<input class=\"blind \" id=\"spoilerYnCheckBox\" name=\"spilerViewer\" onclick=\"parent.clickcr(this,'','','',event); return false;\" title=\"스포일러 보기\" type=\"checkbox\"/>\n",
              "<label class=\"label_viewer\" for=\"spoilerYnCheckBox\" id=\"spoilerYnLable\">스포일러 보기</label>\n",
              "</li>\n",
              "<li>\n",
              "<input class=\"blind \" id=\"actualYnCheckBox\" name=\"viewer\" onclick=\"parent.clickcr(this,'ura.mgs','','',event); return false;\" title=\"관람객 평점만 보기\" type=\"checkbox\"/>\n",
              "<label class=\"label_viewer\" for=\"actualYnCheckBox\" id=\"actualYnLable\">관람객 평점만 보기</label>\n",
              "<a class=\"help _actualPointHelp\" href=\"#\" id=\"actualPointHelpButton\" title=\"도움말\">관람객 평점만 보기 도움말</a>\n",
              "<div class=\"ly_help _actualPointHelp\" id=\"actualPointHelp\" style=\"display:none\">\n",
              "<h5>관람객평점만 보기 안내 레이어</h5>\n",
              "<div class=\"ly_cont _actualPointHelp\">\n",
              "<p>네이버 영화에서 예매하신 고객님들이<br/> 영화 관람 후 등록해주신 평점입니다.</p>\n",
              "</div>\n",
              "<button class=\"btn_close _actualPointHelp\" id=\"actualPointHelpCloseButton\" title=\"닫기\" type=\"button\"><span class=\"blind\">관람객 평점만 보기 안내 레이어 닫기</span></button>\n",
              "<span class=\"arr _actualPointHelp\"></span>\n",
              "</div>\n",
              "</li>\n",
              "</ul>\n",
              "</div>\n",
              "<div class=\"score_result\">\n",
              "<ul>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_0\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t이 영화를 보고 암이 나았습니다. \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7719905, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>maan****</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.07.09 14:25</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','maan****', 'gjTHoYpT+IBSfJd2kYsNhwWvjnNNu6q9enMS7rln1ys=', '이 영화를 보고 암이 나았습니다. ', '7719905', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7719905\">21460</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7719905\">584</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:10.0%\"></span></span><em>1</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_1\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t이것은절대1점이아니다11점을주고싶은 내마음이다 \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7706096, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>joke****</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.07.05 01:41</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','joke****', '4ouozOv4JvbTvnkmuLc3lk/ZckTQzEfmv2CWFTduKbE=', '이것은절대1점이아니다11점을주고싶은 내마음이다 ', '7706096', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7706096\">18044</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7706096\">800</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_2\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t모니터도 울고 외장하드도 울고 숨어있던 바이러스도 울었다 \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7558782, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>샤브레(dccm****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.05.27 20:57</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','dccm****', 'ebq5MxV2nZpaU5eOoeqdZKUH5KTyjISbtfLzvNcsVmA=', '모니터도 울고 외장하드도 울고 숨어있던 바이러스도 울었다 ', '7558782', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7558782\">13816</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7558782\">324</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_3\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t당신이 이 영화를 보지 않았다면  아직 살아있을 이유 하나를 간직하고 있는 것이다. \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7582795, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>그라운드(hank****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.06.05 18:20</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','hank****', 'BRf0mzdp//dShGop9MGUKAdyOlskqosNfizt1bxYEG8=', '당신이 이 영화를 보지 않았다면  아직 살아있을 이유 하나를 간직하고 있는 것이다. ', '7582795', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7582795\">12846</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7582795\">372</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_4\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t영화계엔 BC와 AC가 있다. Before Clementain, After Clementain... \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7645656, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>Kyle(pool****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.06.16 01:38</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','pool****', '3qOlYY3xCafJ1nuv+wIfBsLIWr109wGdhCWkSh8U+3U=', '영화계엔 BC와 AC가 있다. Before Clementain, After Clementain... ', '7645656', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7645656\">10330</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7645656\">236</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_5\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t남친 집에서 클레멘타인 DVD를 발견했고, 결혼을 결심했습니다. \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(8519064, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>미단스(mida****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2014.02.18 17:00</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','mida****', 'niVHoyXsctNhou8KuD1Lq6seGI/Ufi3Y6yK8epYZnm0=', '남친 집에서 클레멘타인 DVD를 발견했고, 결혼을 결심했습니다. ', '8519064', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_8519064\">10083</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_8519064\">256</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_6\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t드디어 보았네요. 나이 40대 중반에 이런 감동을 느낄 수 있음에 스스로도 대견한 생각이 듭니다. 먼 훗날 제 아이들도 이 영화를 보고 저와 같은 감동을 느끼길 바라는 건 그저 바램이겠지요? \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7717821, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>통달선생(hars****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.07.08 17:14</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','hars****', '1dwVudkQp79OtpSKQL8pjNKc4lZ21psrbjGNNhvUMvU=', '드디어 보았네요. 나이 40대 중반에 이런 감동을 느낄 수 있음에 스스로도 대견한 생각이 듭니다. 먼 훗날 제 아이들도 이 영화를 보고 저와 같은 감동을 느끼길 바라는 건 그저 바램이겠지요? ', '7717821', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7717821\">8924</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7717821\">317</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_7\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t평점깍아내리는 낚시글 몇개있는데 제발그만해라 이기적인새끼들아좋은건 나누는거다 \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7660079, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>와라이탁(qna0****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.06.20 18:36</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','qna0****', 'fo9fulmTxmCBxydE4LZdMZxb9nGqwEfOZlXmkfY5wMA=', '평점깍아내리는 낚시글 몇개있는데 제발그만해라 이기적인새끼들아좋은건 나누는거다 ', '7660079', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7660079\">8276</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7660079\">291</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_8\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t장난으로 쓰지마라... 본인은 2004년에 실제로 극장에서 이 영화를 봤다 \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(8922451, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>소이탄(tjrg****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2014.07.17 00:31</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','tjrg****', 'pmEJvJiQYsc0co5rTl2jimCORfJKK9NxbHngVQq9r8o=', '장난으로 쓰지마라... 본인은 2004년에 실제로 극장에서 이 영화를 봤다 ', '8922451', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_8922451\">7152</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_8922451\">143</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "<li class=\"last\">\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_9\">\n",
              "<span class=\"_unfold_ment\" id=\"_unfold_ment9\">\n",
              "<a data-src=\"프로포즈 선물로 다이아반지 대신 클레멘타인 파일을 USB에담아 목걸이로 만들어 그녀 목에 걸어주었다. 눈물로 대신 대답한 그녀...그리고 2세이름은 그녀의 의견을 반영해 이동준과 스티븐시갈의 이름을 딴 이동갈로 지을 예정이다. \" href=\"javascript:void(0);\" onclick=\"unfoldPointMent(this);\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t프로포즈 선물로 다이아반지 대신 클레멘타인 파일을 USB에담아 목걸이로 만들어 그녀 목에 걸어주었다. 눈물로 대신 대답한 그녀...그리고 2세이름은 그녀의 의견을 반영해 이동준과 스티븐시갈의 이름을 딴 이동갈로 지을...\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a>\n",
              "</span>\n",
              "</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(8924525, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>잉잉(beat****)</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2014.07.17 14:49</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','beat****', 'tPJTeTb23/zWz0P6HUBH4ZVJhBF/Ox2Ja+BfhvIrdIw=', '프로포즈 선물로 다이아반지 대신 클레멘타인 파일을 USB에담아 목걸이로 만들어 그녀 목에 걸어주었다. 눈물로 대신 대답한 그녀...그리고 2세이름은 그녀의 의견을 반영해 이동준과 스티븐시갈의 이름을 딴 이동갈로 지을 예정이다. ', '8924525', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_8924525\">6767</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_8924525\">160</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>\n",
              "</ul>\n",
              "</div>\n",
              "<div class=\"paging\">\n",
              "<div>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=1\" id=\"pagerTagAnchor1\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span class=\"on\">1</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=2\" id=\"pagerTagAnchor2\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>2</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=3\" id=\"pagerTagAnchor3\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>3</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=4\" id=\"pagerTagAnchor4\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>4</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=5\" id=\"pagerTagAnchor5\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>5</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=6\" id=\"pagerTagAnchor6\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>6</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=7\" id=\"pagerTagAnchor7\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>7</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=8\" id=\"pagerTagAnchor8\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>8</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=9\" id=\"pagerTagAnchor9\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>9</span></a>\n",
              "<a href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=10\" id=\"pagerTagAnchor10\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\"><span>10</span></a>\n",
              "<a class=\"pg_next\" href=\"/movie/bi/mi/pointWriteFormList.nhn?code=37886&amp;type=after&amp;isActualPointWriteExecute=false&amp;isMileageSubscriptionAlready=false&amp;isMileageSubscriptionReject=false&amp;page=2\" id=\"pagerTagAnchor2\" onclick=\"parent.clickcr(this, 'ara.page', '', '', event);\" title=\"다음\"><em>다음</em></a>\n",
              "</div>\n",
              "</div>\n",
              "</div>\n",
              "</div>\n",
              "<!-- //content -->\n",
              "<form accept-charset=\"euc-kr\" id=\"reportForm\" method=\"POST\" name=\"reportForm1\"></form>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "\n",
              "if (false == false && \"after\" == \"after\" && false) {\n",
              "\tif (true && false) {\n",
              "\t\tjindo.$Element(\"pointWriteExecuteLayer\").show();\n",
              "\t} else if (false && true) {\n",
              "\t\tjindo.$Element(\"actualPointWriteExecuteLayer\").show();\n",
              "\t}\n",
              "}\n",
              "\n",
              "var oElActualPointWriteExecuteLayer = jindo.$Element(\"actualPointWriteExecuteLayer\");\n",
              "\n",
              "if (oElActualPointWriteExecuteLayer != null && oElActualPointWriteExecuteLayer != \"undefined\") {\n",
              "\n",
              "\t\n",
              "\tjindo.$Element(\"actualPointWriteExecuteLayerOkButton\").attach(\"click\", function(e){\n",
              "\t\te.stop(jindo.$Event.CANCEL_DEFAULT);\n",
              "\t\tjindo.$Element(\"actualPointWriteExecuteLayer\").hide();\n",
              "\t\trefreshPage();\n",
              "\t});\n",
              "\t\n",
              "\t\n",
              "\t\n",
              "\tjindo.$Element(\"actualPointWriteExecuteLayerCloseButton\").attach(\"click\", function(e){\n",
              "\t\te.stop(jindo.$Event.CANCEL_DEFAULT);\n",
              "\t\tjindo.$Element(\"actualPointWriteExecuteLayer\").hide();\n",
              "\t\trefreshPage();\n",
              "\t});\n",
              "\t\n",
              "}\n",
              "\n",
              "var oElPointWriteExecuteLayer = jindo.$Element(\"pointWriteExecuteLayer\");\n",
              "\n",
              "if (oElPointWriteExecuteLayer != null && oElPointWriteExecuteLayer != \"undefined\") {\n",
              "\t\n",
              "\tjindo.$Element(\"pointWriteExecuteLayerOkButton\").attach(\"click\", function(e){\n",
              "\t\te.stop(jindo.$Event.CANCEL_DEFAULT);\n",
              "\t\tjindo.$Element(\"pointWriteExecuteLayer\").hide();\n",
              "\t\trefreshPage();\n",
              "\t});\n",
              "\t\n",
              "\t\n",
              "\t\n",
              "\tjindo.$Element(\"pointWriteExecuteLayerCloseButton\").attach(\"click\", function(e){\n",
              "\t\te.stop(jindo.$Event.CANCEL_DEFAULT);\n",
              "\t\tjindo.$Element(\"pointWriteExecuteLayer\").hide();\n",
              "\t\trefreshPage();\n",
              "\t});\n",
              "\t\n",
              "}\n",
              "\n",
              "jindo.$Fn(function () {\n",
              "\n",
              "  function checkboxHandlerFactory(isActualYn, isLabel) {\n",
              "    return function() {\n",
              "      var actualYnChecked = jindo.$(\"actualYnCheckBox\").checked;\n",
              "      var spoilerYnChecked = jindo.$(\"spoilerYnCheckBox\").checked;\n",
              "\n",
              "      // convert\n",
              "      actualYnChecked = (isActualYn && isLabel)? !actualYnChecked : actualYnChecked;\n",
              "      spoilerYnChecked = (!isActualYn && isLabel)? !spoilerYnChecked : spoilerYnChecked;\n",
              "\n",
              "      location.href = \"/movie/bi/mi/pointWriteFormList.nhn?code=37886&type=after&onlyActualPointYn=\" + (actualYnChecked? \"Y\" : \"N\")  + \"&onlySpoilerPointYn=\" + (spoilerYnChecked? \"Y\" : \"N\") + \"&order=sympathyScore\";\n",
              "    };\n",
              "  }\n",
              "\n",
              "\t\n",
              "\tif (jindo.$(\"actualYnCheckBox\") != null) {\n",
              "\t\tjindo.$Fn(checkboxHandlerFactory(true, false), this).attach(jindo.$(\"actualYnCheckBox\"), 'click');\n",
              "\t}\n",
              "\n",
              "\tif (jindo.$(\"actualYnLable\") != null) {\n",
              "\t    jindo.$Fn(checkboxHandlerFactory(true, true), this).attach(jindo.$(\"actualYnLable\"), 'click');\n",
              "\t}\n",
              "\n",
              "\t\n",
              "    if (jindo.$(\"spoilerYnCheckBox\") != null) {\n",
              "    jindo.$Fn(checkboxHandlerFactory(false, false), this).attach(jindo.$(\"spoilerYnCheckBox\"), 'click');\n",
              "  }\n",
              "\n",
              "  if (jindo.$(\"spoilerYnLable\") != null) {\n",
              "    jindo.$Fn(checkboxHandlerFactory(false, true), this).attach(jindo.$(\"spoilerYnLable\"), 'click');\n",
              "  }\n",
              "\t\n",
              "\t\n",
              "\tif (jindo.$Element(\"actualPointHelp\") != null && jindo.$Element(\"actualPointHelp\") != \"undefined\") {\n",
              "\t\tactualPointHelpLayerToggle = function() {\n",
              "\t\t\tsetTimeout( function() {\n",
              "\t\t\t\tif (document.activeElement != null) {\n",
              "\t\t\t\t\tvar focusedEl = jindo.$Element(document.activeElement);\n",
              "\t\t\t\t\tif (focusedEl != null) {\n",
              "\t\t\t\t\t\tif ( !focusedEl.hasClass(\"_actualPointHelp\") ) {\n",
              "\t\t\t\t\t\t\t jindo.$Element(\"actualPointHelp\").hide();\n",
              "\t\t\t\t\t\t}\n",
              "\t\t\t\t\t}\n",
              "\t\t\t\t}\n",
              "\t\t\t}, 100);\n",
              "\t\t};\n",
              "\n",
              "\t\tnew jindo.LayerManager(\"actualPointHelp\", {\n",
              "\t\t\tsCheckEvent : \"click\",\n",
              "\t\t\tnHideDelay : 0\n",
              "\t\t}).link(jindo.$(\"actualPointHelp\"), jindo.$(\"actualPointHelpButton\"));\n",
              "\n",
              "\t\tjindo.$Element(\"actualPointHelpButton\").attach(\"click\", function(e){\n",
              "\t\t\te.stop(jindo.$Event.CANCEL_DEFAULT);\n",
              "\t\t\tjindo.$Element(\"actualPointHelp\").toggle();\n",
              "\t\t});\n",
              "\n",
              "\t\tjindo.$Element(\"actualPointHelpCloseButton\").attach(\"click\", function(e){\n",
              "\t\t\te.stop(jindo.$Event.CANCEL_DEFAULT);\n",
              "\t\t\tjindo.$Element(\"actualPointHelp\").toggle();\n",
              "\t\t});\n",
              "\n",
              "\t\t\n",
              "\t\tvar waelActualPointHelp = jindo.$ElementList('._actualPointHelp');\n",
              "\n",
              "\t\tjindo.$A(waelActualPointHelp.$value()).forEach(function(value, index, array) {\n",
              "\t\t\tjindo.$Fn(actualPointHelpLayerToggle, this).attach(value, \"blur\");\n",
              "\t\t});\n",
              "\t}\n",
              "\n",
              "\tparent.setParamForPointAfterList('N', 'sympathyScore', '1');\n",
              "\tparent.resizePointAfterListIframe(0);\n",
              "\n",
              "\t// 최소 높이 270px 지정\n",
              "\tvar frameHeight = eval(jindo.$Document().scrollSize().height);\n",
              "\n",
              "\tparent.resizePointAfterListIframe(frameHeight);\n",
              "\tparent.isPointAfterListLoad = true;\n",
              "\t\n",
              "\t\n",
              "\t\n",
              "\n",
              "  var isCheckPointExist = false;\n",
              "  // TODO: fix below condition\n",
              "  if (false) {\n",
              "    if(jindo.$Element(\"open-form-btn\")) {\n",
              "\t\tjindo.$Element(\"open-form-btn\").attach(\"click\", function(e) {\n",
              "\t\t  \t\n",
              "\t\t  \t\n",
              "\t\t\t\n",
              "\t\t  if (true && false && isCheckPointExist == false) {\n",
              "\t\t\tparent.point.checkPointAfterExistAndMileageSubscriptionType();\n",
              "\t\t  }\n",
              "\t\t  isCheckPointExist = true;\n",
              "\t\t  parent.openPointWriteForm();\n",
              "\t\t});\n",
              "    }\n",
              "  } else {\n",
              "    if (jindo.$Element(\"open-form-btn\")) {\n",
              "\t\tjindo.$Element(\"open-form-btn\").attach(\"click\", function (e) {\n",
              "\t\t  common.checkLogin(false);\n",
              "\t\t});\n",
              "    }\n",
              "  }\n",
              "\n",
              "  var isHide = true;\n",
              "\n",
              "  if(jindo.$Element(\"eval-edit\")) {\n",
              "    jindo.$Element(\"eval-edit\").attach(\"click\", function() {\n",
              "\t\t  var edit = jindo.$Element(\"ly-edit\");\n",
              "\t  \tisHide ? edit.show() : edit.hide();\n",
              "      \tisHide = !isHide;\n",
              "    })\n",
              "  }\n",
              "  parent.resizePointAfterListIframeOnLoad();\n",
              "}, this).attach(this, 'load');\n",
              "\n",
              "var point = {\n",
              "\t\tcheckLoginWithMessage : function(login, loginMessage, notLoginMessage) {\n",
              "\t\t\tif(login == false){\n",
              "\t\t\t\tif(confirm(message)){\n",
              "\t\t\t\t\ttop.location.href=\"https://nid.naver.com/nidlogin.login?mode=form&url=\"+encodeURIComponent(top.location.href);\n",
              "\t\t\t\t}\n",
              "\t\t\t\treturn false;\n",
              "\t\t\t}\n",
              "\t\t\treturn true;\n",
              "\t\t},\n",
              "\n",
              "\t\t\n",
              "\t\tcheckAlreadyPointAfterExist : function (nid) {\n",
              "\t\t\tvar existPointType = \"pointBefore\";\n",
              "\t\t\t\n",
              "\t\t\tif (false == false) {\n",
              "\t\t\t\tvar oAjax = new jindo.$Ajax(\"/api/internal/point/pointAfterExistJson.nhn\", {\n",
              "\t\t\t    \tonload : function (oRes) {\n",
              "\t\t\t    \t\tvar resultCode = oRes.json().resultCode;\n",
              "\t\t\t    \t\t\n",
              "\t\t\t    \t\tif (resultCode == \"error\") {\t\t\t\t\t\t\t// 서버 오류\n",
              "\t\t\t    \t\t\talert(\"오류가 발생했습니다. 잠시 후 다시 시도해주세요.\");\n",
              "\t\t\t    \t\t\treturn false;\n",
              "\t\t\t    \t\t} else {\n",
              "\t\t\t    \t\t\texistPointType = oRes.json().existPointType;\n",
              "\t\t\t    \t\t\tpoint.del(existPointType, nid);\n",
              "\t\t\t    \t\t}\n",
              "\t\t\t    \t},\n",
              "\t\t\t\t\ttimeout : 5,\n",
              "\t\t\t    \tonerror : function (oRes) {\n",
              "\t\t\t    \t\talert(\"오류가 발생했습니다. 잠시 후 다시 시도해주세요.\");\n",
              "\t\t\t    \t\treturn false;\n",
              "\t\t\t    \t},\n",
              "\t\t\t    \tontimeout : function (oRes) {\n",
              "\t\t\t    \t\talert(\"처리가 지연되고 있습니다. 다시 시도해주세요.\");\n",
              "\t\t\t    \t\treturn false;\n",
              "\t\t\t    \t}\n",
              "\t\t\t    });\n",
              "\t\t\t    \n",
              "\t\t\t    oAjax.request({\n",
              "\t\t\t    \t\"movieCode\" : \"37886\",\n",
              "\t\t\t    \t\"isActualPoint\" : \"false\"\n",
              "\t\t\t    });\n",
              "\t\t\t} else {\n",
              "\t\t\t\tpoint.del(existPointType, nid);\n",
              "\t\t\t}\n",
              "\t\t},\n",
              "\n",
              "\t\tdel : function (existPointType, nid) {\n",
              "\t\t\tif (existPointType == \"actualPoint\") {\n",
              "\t\t\t\tif (confirm(\"관람객 평점 삭제시, 평점 작성으로 적립된 포인트는 회수됩니다. 평점을 삭제할까요?\") == false) {\n",
              "\t\t\t\t\treturn false;\n",
              "\t\t\t\t}\n",
              "\t\t\t} else {\n",
              "\t\t\t\tif (confirm(\"본인 삭제 시 복구할 수 없습니다.\\n평점을 삭제하시겠습니까?\") == false) {\n",
              "\t\t\t\t\treturn false;\n",
              "\t\t\t\t}\n",
              "\t\t\t}\n",
              "\t\t\t\n",
              "\t\t\tvar ajaxDeleteUrl = \"/api/internal/point/pointBeforeDelete.nhn\";\n",
              "\t\t\tif (\"after\" == \"after\") {\n",
              "\t\t\t\tajaxDeleteUrl = \"/api/internal/point/pointAfterDelete.nhn\";\n",
              "\t\t\t}\n",
              "\t\t\t\n",
              "\t\t\tvar ajax = new jindo.$Ajax(ajaxDeleteUrl, { \n",
              "\t\t\t\tmethod : \"POST\",\n",
              "\t\t\t\tasync : false,\n",
              "\t\t\t\tonload : this.delCallback\n",
              "\t\t\t});\n",
              "\t\t\tajax.header(\"ajax\", \"true\");\n",
              "\t\t\tajax.request({\n",
              "\t\t\t\t\"nid\":nid\n",
              "\t\t\t});\n",
              "\t\t},\n",
              "\t\t\n",
              "\t\tdelCallback : function(req) {\n",
              "\t\t\tvar returnValue = req.text();\n",
              "\t\t\t\n",
              "\t\t\tif(returnValue != \"success\"){\n",
              "\t\t\t\talert(returnValue);\n",
              "\t\t\t\treturn false;\n",
              "\t\t\t}\n",
              "\t\t\t\n",
              "\t\t\ttop.location.href = '/movie/bi/mi/point.nhn?code=37886#pointAfterTab';\n",
              "\t\t\ttop.location.reload(true);\n",
              "\t\t}\n",
              "};\n",
              "\n",
              "function dislplayOrder(order) {\n",
              "\tvar url = \"/movie/bi/mi/pointWriteFormList.nhn?code=37886&type=after\";\n",
              "\t\n",
              "\tvar onlyActualPointYnValue = jindo.$(\"onlyActualPointYn\").value;\n",
              "  \tvar includeSpoilerYnValue = jindo.$(\"includeSpoilerYn\").value;\n",
              "\n",
              "\tif (onlyActualPointYnValue != \"\") {\n",
              "\t\turl = url + \"&onlyActualPointYn=\" + onlyActualPointYnValue;\n",
              "\t}\n",
              "\n",
              "\tif (includeSpoilerYnValue != \"\") {\n",
              "      url = url + \"&onlySpoilerPointYn=\" + includeSpoilerYnValue;\n",
              "    }\n",
              "\t\n",
              "\turl = url + \"&order=\" + order;\n",
              "\t\n",
              "\tlocation.href = document.location.protocol + \"//\" + document.domain + url;\n",
              "}\n",
              "\n",
              "function showPointListByNid(nid, target){\n",
              "\tif (target == 'after') {\n",
              "\t\ttop.location.href = top.location.protocol + \"//\" + top.location.hostname + \"/movie/point/af/list.nhn?st=nickname&target=after&sword=\"+nid;\n",
              "\t} else {\n",
              "\t\ttop.location.href = top.location.protocol + \"//\" + top.location.hostname + \"/movie/point/af/list.nhn?st=nickname_before&target=before&sword=\"+nid;\n",
              "\t}\n",
              "}\n",
              "\n",
              "function refreshPage() {\n",
              "\t \n",
              "\ttop.location.href = '/movie/bi/mi/point.nhn?code=37886#pointAfterTab';\n",
              "}\n",
              "\n",
              "// 스포일러 감상평 내용 보기\n",
              "function showMovieReview(rvwIdx) {\n",
              "    jindo.$Element('_text_spo_' + rvwIdx).css('display', 'none');\n",
              "    jindo.$Element('_filtered_ment_' + rvwIdx).css('display', 'block');\n",
              "    parent.resizePointAfterListIframeOnLoad();\n",
              "}\n",
              "\n",
              "// 감상평 펼쳐보기\n",
              "function unfoldPointMent(obj) {\n",
              "    var fullMent = jindo.$Element(obj).attr(\"data-src\");\n",
              "    jindo.$Element(obj).parent().html(fullMent);\n",
              "    parent.resizePointAfterListIframeOnLoad();\n",
              "}\n",
              "\n",
              "\n",
              "</script>\n",
              "<script type=\"text/javascript\">\n",
              "\tvar sympathy = new Sympathy(\"37886\", \"after\");\n",
              "</script>\n",
              "<script type=\"text/javascript\">\n",
              "            jindo.$Fn(function() {\n",
              "                try{ lcs_do(); } catch(e){}\n",
              "            }).attach(window, \"pageshow\");\n",
              "\t\t</script>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4CwKchipvNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "outputId": "b4fa153c-1637-412a-e387-5c8d4f597af7"
      },
      "source": [
        "# 개발자 도구에서 살펴보니 score_result라는 class안에 각각의 li들이 있었으며 이 li는 'star_score' 'scroe_reple' 'btn_area'의 세부 클래스로 구성이 되어있었음.\n",
        "# 상기 url에서 맨 위에 뜨는 리뷰 '이 영화를 보고 암이 나았습니다' 평점 10이 크롤링한 결과에도 나오는걸 확인\n",
        "\n",
        "score_result = html.find('div', {'class': 'score_result'})\n",
        "lis = score_result.findAll('li')\n",
        "lis[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<li>\n",
              "<div class=\"star_score\">\n",
              "<span class=\"st_off\"><span class=\"st_on\" style=\"width:100.0%\"></span></span><em>10</em>\n",
              "</div>\n",
              "<div class=\"score_reple\">\n",
              "<p>\n",
              "<!-- 스포일러 컨텐츠로 처리되는지 여부 -->\n",
              "<span id=\"_filtered_ment_0\">\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t이 영화를 보고 암이 나았습니다. \n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
              "\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n",
              "</p>\n",
              "<dl>\n",
              "<dt>\n",
              "<em>\n",
              "<a href=\"#\" onclick=\"javascript:showPointListByNid(7719905, 'after');parent.clickcr(this, 'ara.uid', '', '', event); return false;\" target=\"_top\">\n",
              "<span>maan****</span>\n",
              "</a>\n",
              "</em>\n",
              "<em>2013.07.09 14:25</em>\n",
              "</dt>\n",
              "<dd>\n",
              "<a class=\"go_report2\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.report', '', '', event); common.report('false','maan****', 'gjTHoYpT+IBSfJd2kYsNhwWvjnNNu6q9enMS7rln1ys=', '이 영화를 보고 암이 나았습니다. ', '7719905', 'point_after', false);return false;\"><em>신고</em></a>\n",
              "</dd>\n",
              "</dl>\n",
              "</div>\n",
              "<div class=\"btn_area\">\n",
              "<a class=\"_sympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.sym', '', '', event);\">\n",
              "<span class=\"ico_up\"></span>\n",
              "<span class=\"blind\">공감</span>\n",
              "<strong class=\"sympathy_7719905\">21460</strong>\n",
              "</a>\n",
              "<a class=\"_notSympathyButton\" href=\"#\" onclick=\"parent.clickcr(this, 'ara.opp', '', '', event);\">\n",
              "<span class=\"ico_down\"></span>\n",
              "<span class=\"blind\">비공감</span>\n",
              "<strong class=\"notSympathy_7719905\">584</strong>\n",
              "</a>\n",
              "</div>\n",
              "</li>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVJEA4Ueq0Jo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45514ad5-d999-4f57-f1d6-ec050790c9a5"
      },
      "source": [
        "# scroe_reple아래 텍스트 리뷰는 p라는 클래스로 이루어져있음 , 그리고 양끝공백제거 \n",
        "\n",
        "review_text = lis[0].find('p').getText()\n",
        "review_text.strip()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'이 영화를 보고 암이 나았습니다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxRcjZjrriCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f426e5ae-2935-479b-c109-cae2b96eae1a"
      },
      "source": [
        "# 점수확인\n",
        "\n",
        "score = lis[0].find('em').getText()\n",
        "score"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xOhxhD_rq8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "4a67563f-2f99-4ecb-ded8-a11d3771ff4d"
      },
      "source": [
        "for lis_ in lis:\n",
        "    comment = lis_.find('p').getText().strip()\n",
        "    score = lis_.find('em').getText()\n",
        "    print(comment,score)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이 영화를 보고 암이 나았습니다. 10\n",
            "이것은절대1점이아니다11점을주고싶은 내마음이다 1\n",
            "모니터도 울고 외장하드도 울고 숨어있던 바이러스도 울었다 10\n",
            "당신이 이 영화를 보지 않았다면  아직 살아있을 이유 하나를 간직하고 있는 것이다. 10\n",
            "영화계엔 BC와 AC가 있다. Before Clementain, After Clementain... 10\n",
            "남친 집에서 클레멘타인 DVD를 발견했고, 결혼을 결심했습니다. 10\n",
            "드디어 보았네요. 나이 40대 중반에 이런 감동을 느낄 수 있음에 스스로도 대견한 생각이 듭니다. 먼 훗날 제 아이들도 이 영화를 보고 저와 같은 감동을 느끼길 바라는 건 그저 바램이겠지요? 10\n",
            "평점깍아내리는 낚시글 몇개있는데 제발그만해라 이기적인새끼들아좋은건 나누는거다 10\n",
            "장난으로 쓰지마라... 본인은 2004년에 실제로 극장에서 이 영화를 봤다 10\n",
            "프로포즈 선물로 다이아반지 대신 클레멘타인 파일을 USB에담아 목걸이로 만들어 그녀 목에 걸어주었다. 눈물로 대신 대답한 그녀...그리고 2세이름은 그녀의 의견을 반영해 이동준과 스티븐시갈의 이름을 딴 이동갈로 지을... 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE8wS3a6r5jy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "2f5dca80-beb9-4462-da8b-f825eaeb5f15"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "score_df = pd.DataFrame(columns=['comment','score'])\n",
        "score_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [comment, score]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYH6PSUQsB6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9ca168c-c1a7-4b6a-b5e9-2b15fc9d2cee"
      },
      "source": [
        "page_num = 1\n",
        "index = 0\n",
        "\n",
        "#페이지별로 이동하며 모든 리뷰 긁어모음\n",
        "while True:\n",
        "    url = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=37886&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page={}\".format(page_num)\n",
        "    response = requests.get(url)\n",
        "    html = BeautifulSoup(response.content, 'html.parser')\n",
        "    score_result = html.find('div', {'class': 'score_result'})\n",
        "    lis = score_result.findAll('li')\n",
        "\n",
        "    for lis_ in lis:\n",
        "        comment = lis_.find('p').getText().strip().replace(\"\\n\",\"\").replace('\\t','').replace('\\r','') # score_reple클래스의 p에 영화 리뷰가 있음, 줄바꿈 커서  탭이동 맨앞이동을 공백으로 제거 \n",
        "        comment = comment.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") #정규표현식으로 제거 \n",
        "\n",
        "        comment = comment.replace(\"관람객\",\"\")\n",
        "        comment = comment.replace(\"스포일러가 포함된 감상평입니다.\",\"\") \n",
        "        comment = comment.replace(\"감상평 보기\",\"\")\n",
        "\n",
        "        score = lis_.find('em').getText() # star_score 클래스의 em에 점수가 있음\n",
        "        \n",
        "        if ('관람객' in comment) or ('관람객' in score):continue\n",
        "        score_df.loc[index] = [comment,score]\n",
        "\n",
        "        if index%300==0:\n",
        "            print(f'{index}의 리뷰를 탐색함')\n",
        "            print(comment,score)\n",
        "            print('-------------------'*3)\n",
        "        index+=1\n",
        "        \n",
        "    page_num+=1\n",
        "    if index>30000:break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0의 리뷰를 탐색함\n",
            "이 영화를 보고 암이 나았습니다. 10\n",
            "---------------------------------------------------------\n",
            "300의 리뷰를 탐색함\n",
            "남성으로서의 자신감이 부족하여 소변을 앉아서보던 저였습니다만 이영화의 아빠~일어나~! 를 듣는순간 일어서서 소변을 보게되었습니다. 그렇게 새로산 바지를 버렸고 제 안 깊은곳에 나약함도 버렸습니다. 10\n",
            "---------------------------------------------------------\n",
            "600의 리뷰를 탐색함\n",
            "심영에게 이 영화를 보여줬더니 알이 두개 생겼습니다. 10\n",
            "---------------------------------------------------------\n",
            "900의 리뷰를 탐색함\n",
            "평점깍아내리는 낚시글 몇개있는데 제발그만해라 이기적인새끼들아좋은건 나누는거다 10\n",
            "---------------------------------------------------------\n",
            "1200의 리뷰를 탐색함\n",
            "오스카 상을 받아 마땅한 영화, 기생충보다도 평점이 높다 10\n",
            "---------------------------------------------------------\n",
            "1500의 리뷰를 탐색함\n",
            "인생은 B(Birth)와 D(Death) 사이의 C(Clementine)이다 10\n",
            "---------------------------------------------------------\n",
            "1800의 리뷰를 탐색함\n",
            "제가 감히 이 영화를 판단할 자격이될지 모르지만 지금 네이버 담당자에게 건의합니다 10점은 너무 가옥하지않나요? 10\n",
            "---------------------------------------------------------\n",
            "2100의 리뷰를 탐색함\n",
            "저의 인생이 2시간밖에 남지않았다면 저는 클레멘타인을 틀 것 입니다. 10\n",
            "---------------------------------------------------------\n",
            "2400의 리뷰를 탐색함\n",
            "다시봐도 또 보고싶은 명작 10\n",
            "---------------------------------------------------------\n",
            "2700의 리뷰를 탐색함\n",
            "지하철에서 이 영화를 보면서 갔습니다. \"아빠 일어나\" 라는 대목에서 모든사람이 일어나는 광경을 보았습니다. 10\n",
            "---------------------------------------------------------\n",
            "3000의 리뷰를 탐색함\n",
            "이영화는 진짜 시대를 잘못타고 나왔다. 이명작이 관객수가 3000명밖에 안된다니 지금 재개봉하면 msg살짝쳐서 3000만도 넘을수 있을것이다. 10\n",
            "---------------------------------------------------------\n",
            "3300의 리뷰를 탐색함\n",
            "전설의 영화입니다!! 10\n",
            "---------------------------------------------------------\n",
            "3600의 리뷰를 탐색함\n",
            "발렌타인에는 연인과 함께 클레멘타인을... 10\n",
            "---------------------------------------------------------\n",
            "3900의 리뷰를 탐색함\n",
            "이영화를 보지 않고서는 영화를 논하지말라. 10\n",
            "---------------------------------------------------------\n",
            "4200의 리뷰를 탐색함\n",
            "완벽 인생영화 님들 보셈 10\n",
            "---------------------------------------------------------\n",
            "4500의 리뷰를 탐색함\n",
            "신이 인간을 만든이유는 이영화를 만들기위해서가 아니었을까... 10\n",
            "---------------------------------------------------------\n",
            "4800의 리뷰를 탐색함\n",
            "깊은 감동 받았습니다. 10\n",
            "---------------------------------------------------------\n",
            "5100의 리뷰를 탐색함\n",
            "크리스마스날 이영화를 공중파에서 방송한다면 따뜻한 크리스마스가 되지않을까요~ 10\n",
            "---------------------------------------------------------\n",
            "5400의 리뷰를 탐색함\n",
            "도저히 안보고는 못버티겠어서 동참하기 위해서 없는 시간 쪼개서 봤다. 영화 보는 시각이 확실히 넓어진 느낌이다. 10\n",
            "---------------------------------------------------------\n",
            "5700의 리뷰를 탐색함\n",
            "이 영화를  전세계 개봉 하였더라면,눈물이 바다를 이루어수심을 늘릴수 있었을텐데. 10\n",
            "---------------------------------------------------------\n",
            "6000의 리뷰를 탐색함\n",
            "제 눈 따위는 이 영화를 볼 자격이 없습니다 10\n",
            "---------------------------------------------------------\n",
            "6300의 리뷰를 탐색함\n",
            "무슨말이 필요한가....아빠 일어나.. 10\n",
            "---------------------------------------------------------\n",
            "6600의 리뷰를 탐색함\n",
            "오옷.... 이것은..   정말.. 10\n",
            "---------------------------------------------------------\n",
            "6900의 리뷰를 탐색함\n",
            "클레멘타인을 볼 때 준비물: 팝콘, 음료수, 그리고 갈아입을 여분의 팬티 한장 10\n",
            "---------------------------------------------------------\n",
            "7200의 리뷰를 탐색함\n",
            "여자친구랑 DVD방가서 같이봤는데 여친은 눈에들어오지도 않더라고요 감동에 또 감동‥‥가끔 코믹스런 모습에 웃다가 울다가‥여친이랑  눈물만 흘렸습니다‥‥ 눈물흘리려고 간게 아니였는데 그래서 9점드립니다‥ 초강추!! 꿀... 9\n",
            "---------------------------------------------------------\n",
            "7500의 리뷰를 탐색함\n",
            "이 영화로 인해 우울했던 나의 일상이 잠시나마 잊혀 지네요. 10\n",
            "---------------------------------------------------------\n",
            "7800의 리뷰를 탐색함\n",
            "추천합니다. 생각보다 볼만함 10\n",
            "---------------------------------------------------------\n",
            "8100의 리뷰를 탐색함\n",
            "아아 클레멘타인.. 마른 줄 알았던 내 눈물샘에서 눈물이 터질 수 있다는 걸 알게 해준.. 10\n",
            "---------------------------------------------------------\n",
            "8400의 리뷰를 탐색함\n",
            "너무나도 아름다운 영화다. 양치기였던 파리스는 아프로디테가 아니라 이 영화에 사과를 줬어야 했다. 10\n",
            "---------------------------------------------------------\n",
            "8700의 리뷰를 탐색함\n",
            "만세!!! 그가 돌아왔다! 드림팀 이상인과 함께!!!! 10\n",
            "---------------------------------------------------------\n",
            "9000의 리뷰를 탐색함\n",
            "배우 임혁필을 다시볼수있었다 10\n",
            "---------------------------------------------------------\n",
            "9300의 리뷰를 탐색함\n",
            "별점 높게준 평가인들 난 그들을 증오한다... 2\n",
            "---------------------------------------------------------\n",
            "9600의 리뷰를 탐색함\n",
            "진짜 제 인생의 최고의 영화... 컴퓨터에 다운받아놓고 안 지우고 냅두고 있어요. 나중에 한번 또 보려구요 +_+ 10\n",
            "---------------------------------------------------------\n",
            "9900의 리뷰를 탐색함\n",
            "클레멘타인을 보지 못한자 영화를 논할 자격이 없다. 10\n",
            "---------------------------------------------------------\n",
            "10200의 리뷰를 탐색함\n",
            "인생 최고의 영화. 만약 isis의 한명이라도 이 영화를 본다면 그 즉시 그들의 테러행위는 멈출 것입니다. 10\n",
            "---------------------------------------------------------\n",
            "10500의 리뷰를 탐색함\n",
            "나얼 결별후에 부르는 바람기억의 소울보다 더 강력한 소울을 느꼈습니다.. 10\n",
            "---------------------------------------------------------\n",
            "10800의 리뷰를 탐색함\n",
            "이 영화는를 우습게 보지마라...수작이다... 10\n",
            "---------------------------------------------------------\n",
            "11100의 리뷰를 탐색함\n",
            "굿 10\n",
            "---------------------------------------------------------\n",
            "11400의 리뷰를 탐색함\n",
            "영어로 번역해서 유튜브에 올리면 싸이가 세운 조회수 기록이 깨지지 않을까 싶네요. 10\n",
            "---------------------------------------------------------\n",
            "11700의 리뷰를 탐색함\n",
            "인간쓰레기로 살아가고 있는 모든 자들에게 구원과 감동을 선사하는 영화 노문현의 죽음과 함께 가장 큰 감동을 주었던 영화 10\n",
            "---------------------------------------------------------\n",
            "12000의 리뷰를 탐색함\n",
            "한남자의 인생(혹은 똥꼬)을 건 영화다 그것만으로도 10점의 가치가 있는것이 아닌가? 10\n",
            "---------------------------------------------------------\n",
            "12300의 리뷰를 탐색함\n",
            "은서우양의 신들린듯한 연기, 그녀는 지금 어떤모습일 10\n",
            "---------------------------------------------------------\n",
            "12600의 리뷰를 탐색함\n",
            "단순한 신파극액션이라고? 이 영화를 보고서나  씨부려라. 10\n",
            "---------------------------------------------------------\n",
            "12900의 리뷰를 탐색함\n",
            "감동이 쓰나미 처럼 몰려옵니다... 꼭 보세요. 10\n",
            "---------------------------------------------------------\n",
            "13200의 리뷰를 탐색함\n",
            "이영화를 보지않고선 7광구를 봐선안된다.. 10\n",
            "---------------------------------------------------------\n",
            "13500의 리뷰를 탐색함\n",
            "나는 100점을 주고싶다... 10\n",
            "---------------------------------------------------------\n",
            "13800의 리뷰를 탐색함\n",
            "좀만 늦게 개봉했으면 아바타보다 더 큰 돌풍을 일으켰을텐데.. 최고의 명작이다!! 10\n",
            "---------------------------------------------------------\n",
            "14100의 리뷰를 탐색함\n",
            "이 영화는 빌게이츠 가 직접 챙겨볼 영화다. 10\n",
            "---------------------------------------------------------\n",
            "14400의 리뷰를 탐색함\n",
            "꼭 봐야할 영화 10\n",
            "---------------------------------------------------------\n",
            "14700의 리뷰를 탐색함\n",
            "아.. 나는 왜 이영화를 이제야 보게되었을까. 이영화를모르고지낸 7년은 잃어버린 7년이었다.. 10\n",
            "---------------------------------------------------------\n",
            "15000의 리뷰를 탐색함\n",
            "아름답다... 10\n",
            "---------------------------------------------------------\n",
            "15300의 리뷰를 탐색함\n",
            "삶에 지쳐 감성이 매마른사람,  태생이 악한 사람도 이 영화를 본 순간 눈물을 흘린다 10\n",
            "---------------------------------------------------------\n",
            "15600의 리뷰를 탐색함\n",
            "보지는 않았지만 10점~ 왠지 10점이어야할것만같아~ 10\n",
            "---------------------------------------------------------\n",
            "15900의 리뷰를 탐색함\n",
            "터미네이터이후 이런 액션은 처음이다! 10\n",
            "---------------------------------------------------------\n",
            "16200의 리뷰를 탐색함\n",
            "내생애 최고의영화 \"아빠일어나!\" 일주일 내내 쳐울게 만든 그 영화!! 스티븐씨발 10\n",
            "---------------------------------------------------------\n",
            "16500의 리뷰를 탐색함\n",
            "4점대영화가~~알바들아니들이고생이많다얼마받니?10원내가20원더올려줄까 1\n",
            "---------------------------------------------------------\n",
            "16800의 리뷰를 탐색함\n",
            "마지막에 진짜 감동작살~! 10\n",
            "---------------------------------------------------------\n",
            "17100의 리뷰를 탐색함\n",
            "아~아~~~~~!!!!!!!!!!!!!!!!!!!!!!! 말을해야하는가??? 10\n",
            "---------------------------------------------------------\n",
            "17400의 리뷰를 탐색함\n",
            "10점만점에 10점 10\n",
            "---------------------------------------------------------\n",
            "17700의 리뷰를 탐색함\n",
            "클레멘타인을 보고나니 인생의 용기를 얻었습니다. 10\n",
            "---------------------------------------------------------\n",
            "18000의 리뷰를 탐색함\n",
            "대박 10\n",
            "---------------------------------------------------------\n",
            "18300의 리뷰를 탐색함\n",
            "꼬마의 눈물연기 최고였습니다 같이 울었음따ㅠ,.ㅠ망한이유는 시걸캐스팅비? 9\n",
            "---------------------------------------------------------\n",
            "18600의 리뷰를 탐색함\n",
            "이영화를보고서이런영화가 앞으로 몇개나 나올까 생각했습니다 10\n",
            "---------------------------------------------------------\n",
            "18900의 리뷰를 탐색함\n",
            "왕의 남자가 재미였다면 클레멘타인은 감동이다. 10\n",
            "---------------------------------------------------------\n",
            "19200의 리뷰를 탐색함\n",
            "이 영화를 본후 나는 영화를 발로봤다는 생각에 눈물을 흘렸다 10\n",
            "---------------------------------------------------------\n",
            "19500의 리뷰를 탐색함\n",
            "죽어서도 다시 보고싶은 영화입니다. 10\n",
            "---------------------------------------------------------\n",
            "19800의 리뷰를 탐색함\n",
            "10년도 안된 영화인데 마치 갓 영화가 태동했을 때를 보는 듯한 기분??? 1\n",
            "---------------------------------------------------------\n",
            "20100의 리뷰를 탐색함\n",
            "강추...!!!!~ 10\n",
            "---------------------------------------------------------\n",
            "20400의 리뷰를 탐색함\n",
            "재밌다.. 10\n",
            "---------------------------------------------------------\n",
            "20700의 리뷰를 탐색함\n",
            "성지순례 왔습니다!! 10\n",
            "---------------------------------------------------------\n",
            "21000의 리뷰를 탐색함\n",
            "스티븐 시걸이 맞는 영화도 있구나 하며 웃을 수 있게 해주죠^^ 1\n",
            "---------------------------------------------------------\n",
            "21300의 리뷰를 탐색함\n",
            "네이버알바가 있다는 증거가될 영화네. 이런 쓰레기같은 영화가 평점 9.25라니 젠장 ㅋㅋ 중간까지 무서운 인내심으로 보다가 도데체 어디서부터가 9점대다운게 나오나 빠르게 돌려보고 거침없이 꺼버림. 이딴건 예전같았으면... 1\n",
            "---------------------------------------------------------\n",
            "21600의 리뷰를 탐색함\n",
            "진짜 평점을 안 남길수가 없다. 완전 사기 당한 기분이다. 어처구니가 없어서 혼자 빵 터짐. 어릴 적 봤던 심형래 주연의 내일은 참피온이 생각 난다 ㅋㅋㅋ 젠장 ㅋㅋ 평점 보고 혹시나 해서 봤지만 역시나 ㅋㅋㅋㅋ 1\n",
            "---------------------------------------------------------\n",
            "21900의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "22200의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "22500의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "22800의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "23100의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "23400의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "23700의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "24000의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "24300의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "24600의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "24900의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "25200의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "25500의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "25800의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "26100의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "26400의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "26700의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "27000의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "27300의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "27600의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "27900의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "28200의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "28500의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "28800의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "29100의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "29400의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "29700의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n",
            "30000의 리뷰를 탐색함\n",
            "한남충들 영화보는 수준 대단하다 ㄷㄷㄷ 1\n",
            "---------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B75l1chdugxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "95303c55-a9ff-41f8-fd5f-3a49dda97657"
      },
      "source": [
        "score_df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>이 영화를 보고 암이 나았습니다.</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>이것은절대1점이아니다11점을주고싶은 내마음이다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>모니터도 울고 외장하드도 울고 숨어있던 바이러스도 울었다</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>당신이 이 영화를 보지 않았다면  아직 살아있을 이유 하나를 간직하고 있는 것이다.</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>영화계엔 BC와 AC가 있다. Before Clementain, After Clem...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment score\n",
              "0                                 이 영화를 보고 암이 나았습니다.    10\n",
              "1                          이것은절대1점이아니다11점을주고싶은 내마음이다     1\n",
              "2                    모니터도 울고 외장하드도 울고 숨어있던 바이러스도 울었다    10\n",
              "3     당신이 이 영화를 보지 않았다면  아직 살아있을 이유 하나를 간직하고 있는 것이다.    10\n",
              "4  영화계엔 BC와 AC가 있다. Before Clementain, After Clem...    10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KLwVSkyuvpa",
        "colab_type": "text"
      },
      "source": [
        "리뷰 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQl2-VKu8nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a55f4a15-3f6e-44f7-a463-eeaf0c9418d3"
      },
      "source": [
        "# Mecab\n",
        "\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 60 (delta 23), reused 20 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.2MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.4)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/9b/e115101a833605b3c0e6f3a2bc1f285c95aaa1d93ab808314ca1bde63eed/JPype1-0.7.5-cp36-cp36m-manylinux2010_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, tweepy, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-0.7.5 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2020-06-01 04:01:15--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.2, 18.205.93.1, 18.205.93.0, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=XV%2F6n0gw2FeXqpnKmwP64Kf7RJE%3D&Expires=1590985876&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22 [following]\n",
            "--2020-06-01 04:01:16--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=XV%2F6n0gw2FeXqpnKmwP64Kf7RJE%3D&Expires=1590985876&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.18.140\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.18.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.39MB/s    in 0.4s    \n",
            "\n",
            "2020-06-01 04:01:16 (3.39 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2020-06-01 04:02:32--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.0, 18.205.93.1, 18.205.93.2, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=5fHVDGukeOndh2gzSjopDMiGwDw%3D&Expires=1590985301&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22 [following]\n",
            "--2020-06-01 04:02:33--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=5fHVDGukeOndh2gzSjopDMiGwDw%3D&Expires=1590985301&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.224.120\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.224.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  27.5MB/s    in 1.7s    \n",
            "\n",
            "2020-06-01 04:02:35 (27.5 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcddwOcnuwvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stopwords=['아','후','아이구','어','나','우리','저희','을','를','에','의','가','으로','로','에게','저','도','다','등','제','이','은','들','는','좀','잘','걍','과','도'\n",
        ",'를','자','와','한','하다'] #한국말 불용어 \n",
        "\n",
        "tokenizer = Mecab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrbY7lLvEtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f552b73-0c21-46af-9f74-ed8aadb30a78"
      },
      "source": [
        "# 패딩과 언노운때문에 2가지추가 추가됨. 30001개\n",
        "\n",
        "print('전체 샘플의 수 : {}'.format(len(score_df)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 샘플의 수 : 30001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtKsT1UrvJ-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "61f4e7fe-a619-4dd7-8566-1a722490f52a"
      },
      "source": [
        "temp = tokenizer.morphs(score_df.loc[2,'comment'])\n",
        "print(temp) #형태소 분석\n",
        "\n",
        "temp = [word for word in temp if not word in stopwords]\n",
        "print(temp) #불용어 제거확인"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['모니터', '도', '울', '고', '외장', '하드', '도', '울', '고', '숨', '어', '있', '던', '바이러스', '도', '울', '었', '다']\n",
            "['모니터', '울', '고', '외장', '하드', '울', '고', '숨', '어', '있', '던', '바이러스', '울', '었', '다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MKjJWzEvJ8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized=[]\n",
        "for sentence in score_df['comment']:\n",
        "    temp = []\n",
        "    temp = tokenizer.morphs(sentence) # 토큰화\n",
        "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
        "    tokenized.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOphjaNvJ51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c17e039f-ae2c-4d02-bfd4-3384fcb46bec"
      },
      "source": [
        "print(tokenized[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['영화', '보', '고', '암', '나', '았', '습니다', '.'], ['이것', '절대', '1', '점', '아니', '다', '11', '점', '을', '주', '고', '싶', '내', '마음', '다'], ['모니터', '울', '고', '외장', '하드', '울', '고', '숨', '어', '있', '던', '바이러스', '울', '었', '다'], ['당신', '영화', '보', '지', '않', '았', '다면', '아직', '살', '아', '있', '을', '이유', '하나', '간직', '하', '고', '있', '것', '다', '.'], ['영화', '계', '엔', 'BC', 'AC', '있', '다', '.', 'Before', 'Clementain', ',', 'After', 'Clementain', '.', '..'], ['남친', '집', '에서', '클레멘타인', 'DVD', '발견', '했', '고', ',', '결혼', '을', '결심', '했', '습니다', '.'], ['드디어', '보', '았', '네요', '.', '나이', '40', '대', '중반', '이런', '감동', '을', '느낄', '수', '있', '음', '스스로', '대견', '생각', '듭니다', '.', '먼', '훗날', '제', '아이', '영화', '보', '고', '저', '같', '감동', '을', '느끼', '길', '바라', '건', '그저', '바램', '겠', '지요', '?'], ['평점', '깍아내리', '낚시', '글', '몇', '개', '있', '는데', '제발', '그만', '해라', '이기', '적', '인', '새끼', '아', '좋', '건', '나누', '거', '다'], ['장난', '쓰', '지', '마', '라', '.', '..', '본인', '2004', '년', '실제로', '극장', '에서', '영화', '봤', '다'], ['프로', '포즈', '선물', '로', '다이아', '반지', '대신', '클레멘타인', '파일', '을', 'USB', '담', '아', '목걸이', '로', '만들', '어', '그녀', '목', '걸', '어', '주', '었', '다', '.', '눈물', '로', '대신', '대답', '그녀', '.', '..', '그리고', '2', '세', '이름', '그녀', '의견', '을', '반영', '해', '이동준', '스티븐', '시갈', '이름', '을', '딴', '동갈', '로', '지', '을', '.', '..']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoNvGMskvJ3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64c9f6e0-e019-4360-bf65-9c088b94d6d4"
      },
      "source": [
        "from nltk import FreqDist\n",
        "vocab = FreqDist(np.hstack(tokenized))\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 15071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD_JQ6F3vJ1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39c171c1-240d-4cb9-ebb1-810d329fd6de"
      },
      "source": [
        "vocab['명작']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1918"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69X2DY6FwdSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "314d3e85-a849-4930-b205-2a1eff6c1ae6"
      },
      "source": [
        "vocab_size = 500\n",
        "\n",
        "# 상위 vocab_size개의 단어만 보존\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npBxu-tRwdP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
        "word_to_index['pad'] = 1\n",
        "word_to_index['unk'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpb5oOwNwdNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded = []\n",
        "for line in tokenized: #입력 데이터에서 1줄씩 문장을 읽음\n",
        "    temp = []\n",
        "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
        "      try:\n",
        "        temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
        "      except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
        "        temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
        "\n",
        "    encoded.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX0FcFSZwdLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b5cc9586-6a1f-432f-ce9b-7b9c308bc43f"
      },
      "source": [
        "print(encoded[:10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 6, 13, 161, 24, 42, 16, 3], [217, 409, 82, 31, 99, 5, 0, 31, 12, 55, 13, 65, 29, 246, 5], [0, 84, 13, 0, 0, 84, 13, 444, 57, 17, 75, 0, 84, 23, 5], [233, 4, 6, 18, 36, 42, 108, 83, 116, 43, 17, 12, 165, 156, 0, 7, 13, 17, 32, 5, 3], [4, 208, 250, 0, 0, 17, 5, 3, 0, 0, 25, 0, 0, 3, 15], [0, 324, 50, 20, 282, 0, 61, 13, 25, 0, 12, 0, 61, 16, 3], [0, 6, 42, 44, 3, 455, 0, 169, 0, 54, 28, 12, 356, 26, 17, 135, 0, 0, 129, 0, 3, 0, 0, 94, 468, 4, 6, 13, 102, 76, 28, 12, 279, 147, 0, 252, 491, 0, 89, 0, 35], [38, 0, 267, 418, 370, 146, 17, 69, 0, 456, 0, 0, 71, 86, 0, 43, 119, 252, 0, 100, 5], [388, 274, 18, 249, 80, 3, 15, 0, 293, 67, 0, 295, 50, 4, 49, 5], [0, 0, 0, 33, 0, 0, 0, 20, 0, 12, 0, 0, 43, 0, 33, 132, 57, 0, 0, 188, 57, 55, 23, 5, 3, 37, 33, 0, 0, 0, 3, 15, 189, 154, 265, 0, 0, 0, 12, 0, 81, 200, 174, 457, 0, 12, 0, 0, 33, 18, 12, 3, 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSV4qesXwdI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "3ba35d84-a46b-4600-91eb-c61b910c79a0"
      },
      "source": [
        "max_len = max(len(l) for l in encoded)\n",
        "\n",
        "print('리뷰의 최대 길이 : %d' % max_len)\n",
        "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
        "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n",
        "\n",
        "plt.hist([len(s) for s in encoded], bins=50)\n",
        "plt.xlabel('length of sample')\n",
        "plt.ylabel('number of sample')\n",
        "plt.show()\n",
        "\n",
        "# 자연어의 경우 최대길이인 81에 맞추어서 채워줘야하므로 모든 길이를 최대길이인 81에 맞추어야함."
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰의 최대 길이 : 81\n",
            "리뷰의 최소 길이 : 0\n",
            "리뷰의 평균 길이 : 13.689544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ0bz95kwdGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in encoded:\n",
        "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
        "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEsXcIvEwjxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c6cf1198-3790-49da-9bde-43946b1327ad"
      },
      "source": [
        "print('리뷰의 최대 길이 : %d' % max(len(l) for l in encoded))\n",
        "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
        "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰의 최대 길이 : 81\n",
            "리뷰의 최소 길이 : 81\n",
            "리뷰의 평균 길이 : 81.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90mS4P90wjuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "95248193-47d9-499b-ba56-9f1bb19e7468"
      },
      "source": [
        "print(encoded[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 6, 13, 161, 24, 42, 16, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekWEgVeVFt_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a19d160-bb46-4d65-8fdf-5b0deb281904"
      },
      "source": [
        "len(score_df)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0TpWKK5wjsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = score_df[:24000]\n",
        "test_df = score_df[24000:]\n",
        "\n",
        "train_df.to_csv(\"train_data.csv\", index=False)\n",
        "test_df.to_csv(\"test_data.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Z_NbmIwjnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data # torchtext.data 임포트\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "# Mecab을 토크나이저로 사용\n",
        "tokenizer = Mecab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdKX91aowjlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필드 정의\n",
        "TEXT = data.Field(sequential=True,\n",
        "                  use_vocab=True,\n",
        "                  tokenize=tokenizer.morphs, # 토크나이저로는 Mecab 사용.\n",
        "                  lower=True,\n",
        "                  batch_first=True,\n",
        "                  fix_length=20)\n",
        "\n",
        "LABEL = data.Field(sequential=False,\n",
        "                   use_vocab=False,\n",
        "                   is_target=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C960YYoDwqDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset\n",
        "train_data, test_data = TabularDataset.splits(\n",
        "    path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
        "    fields=[('comment', TEXT), ('score', LABEL)], skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN9cSABOwp_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8b6d1227-10fc-4a2e-bfc4-4645b3cc7000"
      },
      "source": [
        "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
        "print('테스트 샘플의 개수 : {}'.format(len(test_data)))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플의 개수 : 24000\n",
            "테스트 샘플의 개수 : 6001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGw948iuwp-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "300ad68f-87af-4cda-e540-236a5239626e"
      },
      "source": [
        "print(vars(train_data[10]))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'comment': ['내', '가', '가장', '부러운', '사람', '은', '이', '영화', '를', '아직', '보', '지', '않', '은', '사람', '이', '다', '.', '.', '보', '기', '전', '으로', '돌아가', '그때', '에', '느꼈', '던', '감동', '을', '다시', '느끼', '고', '싶', '다', '.', '..'], 'score': '10'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8dBCI2mwp7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "006941aa-c517-43ea-da90-4cd05f4fe8eb"
      },
      "source": [
        "TEXT.build_vocab(train_data, min_freq=1)\n",
        "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 14991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbQfAtcmwjhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ERMDV1zwwXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d44df72-75e4-4f42-dc93-6a8e59a50238"
      },
      "source": [
        "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 2290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Ha8oOxwwVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6a2cb25d-0159-4c60-d319-756e583d36be"
      },
      "source": [
        "print(TEXT.vocab.stoi)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7ff30077a598>, {'<unk>': 0, '<pad>': 1, '.': 2, '이': 3, '영화': 4, '는': 5, '다': 6, '보': 7, '을': 8, 'ㄷ': 9, '의': 10, '하': 11, '를': 12, '고': 13, '에': 14, '들': 15, '!': 16, '가': 17, '은': 18, '..': 19, '습니다': 20, '있': 21, '지': 22, '도': 23, '한': 24, '없': 25, '클레멘타인': 26, '대단': 27, '수준': 28, '최고': 29, '충': 30, '한남': 31, '게': 32, '었': 33, ',': 34, '나': 35, '수': 36, '명작': 37, '감동': 38, '내': 39, '말': 40, '점': 41, '것': 42, '로': 43, '인생': 44, '?': 45, '않': 46, '눈물': 47, '평점': 48, '입니다': 49, '할': 50, '10': 51, '았': 52, '아': 53, '네요': 54, '기': 55, '만': 56, '번': 57, '안': 58, '봤': 59, '에서': 60, '으로': 61, '과': 62, '되': 63, '꼭': 64, '정말': 65, '이런': 66, '주': 67, '전': 68, '어': 69, '한국': 70, '일어나': 71, '아빠': 72, '했': 73, '그': 74, '본': 75, '사람': 76, '싶': 77, '합니다': 78, '년': 79, '...': 80, '는데': 81, '면': 82, '와': 83, '적': 84, '진짜': 85, 'ㅋㅋㅋ': 86, '다시': 87, '던': 88, '같': 89, '볼': 90, '필요': 91, '때': 92, '라': 93, '해': 94, '1': 95, '아직': 96, '울': 97, '알': 98, '인': 99, '너무': 100, '후': 101, '겠': 102, '지만': 103, '왜': 104, '세요': 105, '서': 106, '까지': 107, '제': 108, '에게': 109, 'ㅋㅋ': 110, '~': 111, '아니': 112, '거': 113, '더': 114, '분': 115, '저': 116, '시': 117, '\"': 118, '추천': 119, '작품': 120, '다면': 121, '자': 122, '봐야': 123, '님': 124, \"'\": 125, '지금': 126, '죽': 127, '다는': 128, '한다': 129, '살': 130, '중': 131, '액션': 132, '난': 133, '좋': 134, '신': 135, '두': 136, '라고': 137, '네': 138, 'ㅠㅠ': 139, '야': 140, '였': 141, '받': 142, '생각': 143, '줄': 144, '!!': 145, '만들': 146, '감독': 147, '며': 148, '음': 149, '어요': 150, '처음': 151, '이상': 152, '라는': 153, '못': 154, '감사': 155, '남': 156, '봐라': 157, '시간': 158, '보다': 159, '개': 160, '길': 161, '이거': 162, '일': 163, '다니': 164, '면서': 165, '3': 166, '우리': 167, '2': 168, '준': 169, '하나': 170, '시걸': 171, '재밌': 172, '눈': 173, '대작': 174, '시대': 175, '암': 176, '친구': 177, '모든': 178, '이유': 179, '정도': 180, '이건': 181, '될': 182, '대': 183, '사랑': 184, '전설': 185, '삶': 186, '또': 187, '냐': 188, '스티븐': 189, '9': 190, '평가': 191, '니': 192, '이제': 193, '걸작': 194, '세상': 195, '100': 196, '나오': 197, '해서': 198, '가슴': 199, '인간': 200, '다고': 201, '걸': 202, '그리고': 203, '을까': 204, '끝': 205, '역시': 206, '듯': 207, 'ㅠ': 208, '임': 209, '똥': 210, '오늘': 211, '죠': 212, 'dvd': 213, '역사': 214, '이동준': 215, '대한민국': 216, '남자': 217, '재개봉': 218, '형': 219, '강추': 220, '앞': 221, '어서': 222, '계': 223, '세계': 224, '때문': 225, '이렇게': 226, '함': 227, '된': 228, '연기': 229, '잘': 230, '네이버': 231, '후회': 232, '모르': 233, '이것': 234, '는다': 235, '자체': 236, '봐도': 237, '가장': 238, '뭐': 239, '감히': 240, '으면': 241, '-': 242, '는지': 243, '이후': 244, '같이': 245, '나뉜다': 246, '다가': 247, '마지막': 248, '당신': 249, '없이': 250, '속': 251, '개봉': 252, '꼬': 253, '가족': 254, '배우': 255, '작': 256, '이걸': 257, '함께': 258, '왔': 259, '많': 260, '인가': 261, '5': 262, '마음': 263, '오': 264, '마': 265, '좀': 266, '란': 267, '엔': 268, '건': 269, '시작': 270, '높': 271, '뿐': 272, '만든': 273, '그냥': 274, '여': 275, '위해': 276, '댓글': 277, '순간': 278, '보여': 279, '세기': 280, '스토리': 281, '세': 282, '표현': 283, '낚시': 284, '날': 285, ')': 286, '만점': 287, '(': 288, '요': 289, '상': 290, '는가': 291, '쇼': 292, '쓰': 293, '넘': 294, '째': 295, '느끼': 296, '대박': 297, '모두': 298, '다른': 299, '라면': 300, '위': 301, '타인': 302, '이게': 303, '나라': 304, '바로': 305, '아버지': 306, '아서': 307, '의리': 308, '2004': 309, '성지': 310, '극장': 311, '드립니다': 312, '굿': 313, 'oo': 314, '잊': 315, '이제야': 316, '완벽': 317, '찾': 318, '!!!': 319, '여운': 320, 'ㅜㅜ': 321, '부터': 322, '셨': 323, '믿': 324, '희대': 325, '질': 326, '참': 327, '^^': 328, 'ㅋ': 329, '마다': 330, '인데': 331, '인류': 332, '재': 333, '으리': 334, '존재': 335, '단': 336, '자신': 337, '무슨': 338, '흘리': 339, '나올': 340, '집': 341, '순례': 342, '내내': 343, '수작': 344, '진정': 345, '된다': 346, '클': 347, '여기': 348, '김두영': 349, '논하': 350, '영화사': 351, '별': 352, '어도': 353, '해야': 354, '군요': 355, '기대': 356, '기억': 357, '밖': 358, '성': 359, '아깝': 360, '리얼': 361, '밖에': 362, '한번': 363, '이제서야': 364, '더군요': 365, '큰': 366, '가지': 367, '러': 368, '명': 369, '역작': 370, '하지만': 371, '....': 372, '느낄': 373, '무엇': 374, '바': 375, '어떤': 376, '점수': 377, '나요': 378, '동안': 379, '재미있': 380, '잼': 381, '대한': 382, '혼자': 383, '낚': 384, '네티즌': 385, '몇': 386, '사': 387, '스티븐시걸': 388, '씨': 389, '어야': 390, '웃': 391, '느낌': 392, '~~': 393, '의미': 394, '지구': 395, '처럼': 396, '태어나': 397, '20': 398, '7': 399, '생': 400, '스럽': 401, '보고': 402, '인지': 403, '라도': 404, '먹': 405, '여자': 406, '장난': 407, '치': 408, '21': 409, '멈추': 410, '순': 411, '평': 412, '4': 413, '급': 414, '갑니다': 415, '생애': 416, '이해': 417, '위대': 418, '이나': 419, '역대': 420, '나왔': 421, '소장': 422, '장면': 423, '다운': 424, '긴': 425, '절대': 426, '한가': 427, '관객': 428, '듣': 429, '예술': 430, '못했': 431, '편': 432, '흘렸': 433, '글': 434, '별점': 435, '평생': 436, '데': 437, '봐': 438, '흐르': 439, 'b': 440, '께': 441, '아닌': 442, '많이': 443, '감정': 444, '만큼': 445, '놈': 446, '느꼈': 447, '려고': 448, '쓰레기': 449, '애': 450, '겁니다': 451, '그런': 452, '뒤': 453, '딸': 454, '마세요': 455, '어떻게': 456, '진': 457, '랑': 458, '반전': 459, '타이타닉': 460, 'd': 461, '봅니다': 462, '숨': 463, ':': 464, '간': 465, '감상': 466, '아야': 467, '일단': 468, '구나': 469, '재미': 470, '한다면': 471, '힘들': 472, '/': 473, '그만': 474, '나이': 475, '사실': 476, '시갈': 477, '율': 478, '8': 479, '가치': 480, '달': 481, '만드': 482, '손': 483, '이전': 484, '해도': 485, '돈': 486, '아이': 487, '깊': 488, '대사': 489, '리': 490, '여러분': 491, '자랑': 492, '힘': 493, '30': 494, 'ooo': 495, '못한': 496, '아요': 497, 'ㅎㅎ': 498, '버렸': 499, '솔직히': 500, '엄청난': 501, '진심': 502, '쯤': 503, '놓': 504, '더니': 505, '동준': 506, '박수': 507, '재미없': 508, '감': 509, '그저': 510, '드리': 511, '뛰어넘': 512, '멘': 513, '억': 514, '행복': 515, 'ㅜ': 516, '누가': 517, '느껴': 518, '등': 519, '이미': 520, '한테': 521, 'ㅅ': 522, '국민': 523, '기적': 524, '너무나': 525, '설명': 526, 'clementine': 527, '내일': 528, '상영': 529, '갓': 530, '계속': 531, '기분': 532, '됩니다': 533, '리뷰': 534, '맨': 535, '부족': 536, 'ㅡㅡ': 537, '돌아가': 538, '부모': 539, '선택': 540, '에요': 541, '잃': 542, '어디': 543, '크': 544, '포스터': 545, '하루': 546, '그것': 547, '당할': 548, '맞': 549, '못하': 550, '요즘': 551, '소중': 552, '얼마나': 553, '해라': 554, '낚이': 555, '됐': 556, '띵': 557, '레전드': 558, '실': 559, '씩': 560, '영화관': 561, '제작': 562, '졌': 563, '한마디': 564, '화': 565, '획': 566, '희망': 567, '뭔가': 568, '쇼생크': 569, '영웅': 570, '줬': 571, '탈출': 572, '11': 573, '너': 574, '만약': 575, '아쉽': 576, '으': 577, '환상': 578, '0': 579, '뜨거운': 580, '아름다운': 581, '으리으리': 582, '정신': 583, '한국인': 584, '괜찮': 585, '내생': 586, '본다': 587, '불': 588, '이란': 589, '~!': 590, '거짓말': 591, '꿈': 592, '늦': 593, '무조건': 594, '소름': 595, '소리': 596, '이랑': 597, '읽': 598, '제발': 599, '출연': 600, '충격': 601, '한편': 602, '명대사': 603, '몰랐': 604, '연출': 605, '온': 606, '클레': 607, '드디어': 608, '물': 609, '언제': 610, '이름': 611, '제대로': 612, '줘야': 613, '찍': 614, 'ㅂ': 615, '끝난': 616, '났': 617, '당시': 618, '돋': 619, '몸': 620, '못할': 621, '어느': 622, '이야기': 623, '지난': 624, '~~~': 625, '가능': 626, '관람': 627, '깨달': 628, '낳': 629, '선물': 630, '슬프': 631, '아까운': 632, '여친': 633, '빛': 634, '입': 635, '잇': 636, '준리': 637, '치료': 638, '한다는': 639, 'ㅎ': 640, '고맙': 641, '남기': 642, '명화': 643, '물론': 644, '미국': 645, '생겼': 646, '아들': 647, '열': 648, '최대': 649, '타임': 650, '흥행': 651, '나온': 652, '노': 653, '멸망': 654, '뻔': 655, '영': 656, '오랜만': 657, '완전': 658, '욕': 659, '유일': 660, '은데': 661, '태어난': 662, '드라마': 663, '얻': 664, '울리': 665, '잘못': 666, '.....': 667, ';': 668, '끝나': 669, '매일': 670, '사이': 671, '앗': 672, '잡': 673, '틀': 674, '%': 675, '갈': 676, '그러': 677, '라니': 678, '이러': 679, '태어났': 680, '화려': 681, '갔': 682, '걸려': 683, '담': 684, '따위': 685, '머리': 686, '불후': 687, '소녀': 688, '십': 689, '아도': 690, '아아': 691, '잊혀': 692, '천만': 693, '펑펑': 694, '기립': 695, '나미': 696, '낚였': 697, '내용': 698, '누구': 699, '대부': 700, '드': 701, '엇': 702, '위한': 703, '준다': 704, '직접': 705, '짜리': 706, '태권': 707, '태권도': 708, '감성': 709, '그은': 710, '난다': 711, '는다면': 712, '다음': 713, '미친': 714, '보이': 715, '생각나': 716, '아바타': 717, '아주': 718, '어찌': 719, '용': 720, '조작': 721, '첫': 722, '쵝오': 723, ';;': 724, '궁금': 725, '께서': 726, '도록': 727, '디': 728, '반드시': 729, '발': 730, '버린': 731, '시나리오': 732, '엔딩': 733, '일어났': 734, '조차': 735, '군': 736, '그래도': 737, '낫': 738, '낮': 739, '니다': 740, '당장': 741, '딱': 742, '리메이크': 743, '명불허전': 744, '묻': 745, '밋': 746, '아무': 747, '우주': 748, '을까요': 749, '의심': 750, '이영화': 751, '타': 752, 'ㅇ': 753, '간직': 754, '과연': 755, '니까': 756, '아역': 757, '천': 758, '초': 759, '텐데': 760, '헐리우드': 761, '건지': 762, '귀': 763, '레멘': 764, '마치': 765, '만나': 766, '씬': 767, '우': 768, '원': 769, '잠': 770, '조금': 771, '지나': 772, '짱': 773, '최악': 774, '탄생': 775, '해요': 776, 'c': 777, '그렇': 778, '근데': 779, '나누': 780, '논할': 781, '도대체': 782, '레딧': 783, '바랍니다': 784, '자리': 785, '훌륭': 786, '그렇게': 787, '새': 788, '성공': 789, '알려': 790, '우울': 791, 'ㅡ': 792, '그래서': 793, '레': 794, '배': 795, '새로운': 796, '소문': 797, '순위': 798, '아침': 799, '위해서': 800, '은서우': 801, '자식': 802, '힘든': 803, 'ㅎㅎㅎ': 804, '가시': 805, '결혼': 806, '고민': 807, '교과서': 808, '길래': 809, '깨닫': 810, '동참': 811, '밤': 812, '부끄럽': 813, '비운': 814, '셈': 815, '속편': 816, '슬픈': 817, '암세포': 818, '영광': 819, '영원히': 820, '예고편': 821, '자격': 822, '하늘': 823, '흘린': 824, '굳': 825, '그때': 826, '김보성': 827, '꼽': 828, '단어': 829, '비교': 830, '인가요': 831, '주연': 832, '차': 833, '결심': 834, '그날': 835, '는군요': 836, '대해': 837, '도저히': 838, '마시': 839, '멋진': 840, '발견': 841, '부': 842, '빨리': 843, '슬픔': 844, '싫': 845, '심장': 846, '아카데미': 847, '에선': 848, '연기력': 849, '유명': 850, '이젠': 851, '정말로': 852, '제일': 853, '지요': 854, '탄탄': 855, '평론가': 856, '6': 857, '??': 858, '까': 859, '무': 860, '벌써': 861, '삼': 862, '숨겨진': 863, '스필버그': 864, '어제': 865, '으며': 866, '주저': 867, '40': 868, '고서': 869, '공부': 870, '구': 871, '꺼': 872, '래': 873, '미래': 874, '아닐까': 875, '여서': 876, '월': 877, '으니': 878, '특히': 879, '폭풍': 880, '항상': 881, 'ㄱ': 882, 'ㄴ': 883, '결국': 884, '곤': 885, '노아': 886, '다섯': 887, '따뜻': 888, '망': 889, '새끼': 890, '시절': 891, '영상': 892, '영혼': 893, '영화제': 894, '자살': 895, '지났': 896, '대표': 897, '더라면': 898, '덕분': 899, '들어가': 900, '따': 901, '미': 902, '분명': 903, '살아갈': 904, '싸': 905, '앞서간': 906, '에겐': 907, '온몸': 908, '진실': 909, '칸': 910, '포인트': 911, '15': 912, 'the': 913, '강력': 914, '건가': 915, '구성': 916, '극': 917, '낚일': 918, '다행': 919, '단체': 920, '대로': 921, '려': 922, '본다면': 923, '상상': 924, '스텔라': 925, '아닙니다': 926, '장르': 927, '주인공': 928, '지루': 929, '짓': 930, '학교': 931, '회사': 932, '50': 933, 'ㄹ': 934, '광구': 935, '그러나': 936, '기다리': 937, '나이트': 938, '내인생의': 939, '눈물샘': 940, '느': 941, '능가': 942, '대신': 943, '둘': 944, '막': 945, '먼저': 946, '모니터': 947, '목': 948, '바뀌': 949, '바라': 950, '발전': 951, '버리': 952, '봐서': 953, '부분': 954, '비': 955, '비밀': 956, '안타깝': 957, '올': 958, '이번': 959, '인정': 960, '인터': 961, '자기': 962, '전부': 963, '점주': 964, '진리': 965, '충분': 966, '팔': 967, '햇': 968, '했었': 969, '흘렀': 970, '......': 971, '19': 972, '갑자기': 973, '괜히': 974, '나쁜': 975, '나온다': 976, '놀란': 977, '누군가': 978, '닦': 979, '바꿔': 980, '반성': 981, '방주': 982, '베스트': 983, '봐야지': 984, '셔서': 985, '시험': 986, '아라': 987, '죄': 988, '혁명': 989, '호': 990, '1000': 991, '80': 992, 'ㅠㅠㅠ': 993, '극찬': 994, '납니다': 995, '대체': 996, '더라': 997, '동시': 998, '됬': 999, '뜻': 1000, '로그인': 1001, '몰입': 1002, '보내': 1003, '봄': 1004, '비디오': 1005, '살아가': 1006, '영상미': 1007, '입니까': 1008, '천재': 1009, '테러': 1010, '한다고': 1011, '현재': 1012, '12': 1013, 'a': 1014, 'z': 1015, '곳': 1016, '나았': 1017, '내게': 1018, '다크': 1019, '돌': 1020, '돌려': 1021, '든': 1022, '따름': 1023, '문화유산': 1024, '발기부전': 1025, '쑈': 1026, '아름답': 1027, '얘기': 1028, '오브': 1029, '주말': 1030, '준비': 1031, '진다': 1032, '진지': 1033, '첨': 1034, '최': 1035, '하드': 1036, '~!!': 1037, '가끔': 1038, '국내': 1039, '그래': 1040, '깁니다': 1041, '단언': 1042, '답': 1043, '또한': 1044, '박': 1045, '방': 1046, '사상': 1047, '선생': 1048, '시청': 1049, '아저씨': 1050, '오래': 1051, '올리': 1052, '우연히': 1053, '인해': 1054, '장': 1055, '장난치': 1056, '재림': 1057, '전개': 1058, '접하': 1059, '제목': 1060, '죄송': 1061, '주일': 1062, '주체': 1063, '캐스팅': 1064, '헐리': 1065, '현실': 1066, '!!!!': 1067, 'is': 1068, 'ㅣ': 1069, '가진': 1070, '꿀': 1071, '나뉘': 1072, '냐고': 1073, '뇌': 1074, '됨': 1075, '뛰': 1076, '머': 1077, '멈출': 1078, '모독': 1079, '밑': 1080, '바란다': 1081, '벌떡': 1082, '부럽': 1083, '빼': 1084, '시키': 1085, '얼마': 1086, '옆': 1087, '운다': 1088, '웃음': 1089, '이리': 1090, '진정한': 1091, '초반': 1092, '추억': 1093, '치유': 1094, '하도': 1095, '흘릴': 1096, 'ㅁ': 1097, '개인': 1098, '곧': 1099, '기회': 1100, '낭비': 1101, '노래': 1102, '놔': 1103, '더라도': 1104, '돼': 1105, '로또': 1106, '마저': 1107, '미치': 1108, '반지': 1109, '법': 1110, '볼까': 1111, '사나이': 1112, '서사시': 1113, '선정': 1114, '수많': 1115, '습니까': 1116, '아닌가': 1117, '알바': 1118, '연관': 1119, '완치': 1120, '줄기': 1121, '중간': 1122, '줘도': 1123, '쩐': 1124, '채': 1125, '최초': 1126, '축복': 1127, '컴퓨터': 1128, '투자': 1129, '한심': 1130, '화면': 1131, '흐른다': 1132, 'e': 1133, '겟': 1134, '권': 1135, '기준': 1136, '넓': 1137, '돌아보': 1138, '딩크': 1139, '라서': 1140, '레이': 1141, '매년': 1142, '매우': 1143, '미안': 1144, '반': 1145, '방금': 1146, '백': 1147, '별로': 1148, '보여준': 1149, '뽑': 1150, '사라졌': 1151, '수능': 1152, '쏟': 1153, '어라': 1154, '엄마': 1155, '와이프': 1156, '용기': 1157, '웃겨': 1158, '일어날': 1159, '잖아': 1160, '전혀': 1161, '존나': 1162, '쩌': 1163, '포기': 1164, '할까': 1165, '할리우드': 1166, '_': 1167, 'bc': 1168, 'ㅈ': 1169, '고작': 1170, '그대': 1171, '기록': 1172, '기에': 1173, '긴급조치': 1174, '다르': 1175, '다시금': 1176, '마디': 1177, '뭔지': 1178, '미쳤': 1179, '베': 1180, '불과': 1181, '삽니다': 1182, '스': 1183, '스크린': 1184, '슨': 1185, '악마': 1186, '없인': 1187, '열심히': 1188, '영원': 1189, '올라가': 1190, '올해': 1191, '웃기': 1192, '이루': 1193, '절': 1194, '찬사': 1195, '쳤': 1196, '추석': 1197, '터닝': 1198, '틀림없': 1199, '13': 1200, '90': 1201, 'before': 1202, 'of': 1203, '~~~~': 1204, '갤': 1205, '공감': 1206, '그녀': 1207, '기생충': 1208, '나가': 1209, '넣': 1210, '놀랍': 1211, '느라': 1212, '단연': 1213, '던데': 1214, '된다면': 1215, '라멘': 1216, '만난': 1217, '모습': 1218, '물결': 1219, '병': 1220, '북한': 1221, '선': 1222, '설마': 1223, '세포': 1224, '센스': 1225, '소': 1226, '소원': 1227, '실패': 1228, '아마': 1229, '엄청': 1230, '여러': 1231, '여한': 1232, '예수': 1233, '오르': 1234, '을지': 1235, '읍니다': 1236, '이기': 1237, '일까': 1238, '저희': 1239, '제왕': 1240, '조': 1241, '주무시': 1242, '주변': 1243, '줍니다': 1244, '짜': 1245, '크리스마스': 1246, '통해': 1247, '티': 1248, '피': 1249, '한참': 1250, '.!': 1251, 'i': 1252, 'movie': 1253, '가리': 1254, '걸렸': 1255, '경험': 1256, '교훈': 1257, '그런데': 1258, '깊이': 1259, '나머지': 1260, '내리': 1261, '내요': 1262, '도가니': 1263, '동생': 1264, '떨어지': 1265, '뜨': 1266, '마스터': 1267, '무비': 1268, '문화': 1269, '바이러스': 1270, '블루': 1271, '사기': 1272, '사회': 1273, '새벽': 1274, '설': 1275, '성냥팔이': 1276, '소개': 1277, '시켜': 1278, '아님': 1279, '약': 1280, '오직': 1281, '와서': 1282, '원망': 1283, '음악': 1284, '의문': 1285, '이래': 1286, '자마자': 1287, '전후': 1288, '제작비': 1289, '존경': 1290, '졸작': 1291, '줌': 1292, '줘서': 1293, '지인': 1294, '최근': 1295, '판단': 1296, '피스': 1297, '하염없이': 1298, '합시다': 1299, 'after': 1300, 'birth': 1301, 'tv': 1302, 'ㅠㅜ': 1303, '갖': 1304, '검색': 1305, '계시': 1306, '과거': 1307, '군대': 1308, '내면': 1309, '다신': 1310, '달라졌': 1311, '듯이': 1312, '따라': 1313, '뛰어난': 1314, '려는': 1315, '말씀': 1316, '믿기': 1317, '보단': 1318, '본지': 1319, '셔야': 1320, '스타': 1321, '시민': 1322, '십시오': 1323, '씹': 1324, '아름다워': 1325, '아무리': 1326, '악': 1327, '어떻': 1328, '어머니': 1329, '엉엉': 1330, '엿': 1331, '올드보이': 1332, '올라갈': 1333, '옹': 1334, '외치': 1335, '우와': 1336, '유네스코': 1337, '으나': 1338, '으면서': 1339, '자꾸': 1340, '저주': 1341, '직전': 1342, '천국': 1343, '철학': 1344, '최강': 1345, '캐': 1346, '콧물': 1347, '테이큰': 1348, '학생': 1349, '할렐루야': 1350, '행운': 1351, '홍보': 1352, '환자': 1353, '25': 1354, ';;;': 1355, 'ac': 1356, 'ㅉㅉ': 1357, '간다': 1358, '고생': 1359, '김': 1360, '김치': 1361, '꾼': 1362, '는데요': 1363, '더욱': 1364, '땐': 1365, '떠나': 1366, '레알': 1367, '루': 1368, '매기': 1369, '뭘': 1370, '벅찬': 1371, '불쌍': 1372, '살래': 1373, '세월': 1374, '수식어': 1375, '쉬': 1376, '실수': 1377, '심': 1378, '아내': 1379, '아무것': 1380, '어깨': 1381, '어떠': 1382, '언제나': 1383, '예전': 1384, '오열': 1385, '와우': 1386, '외계인': 1387, '으로서': 1388, '이럴': 1389, '이야': 1390, '이토록': 1391, '잠시': 1392, '저녁': 1393, '진한': 1394, '차기': 1395, '태어날': 1396, '팬': 1397, '표': 1398, '한동안': 1399, '할머니': 1400, '확인': 1401, '후기': 1402, '힘내': 1403, '.......': 1404, 'death': 1405, 'good': 1406, '감격': 1407, '감명깊': 1408, '견줄': 1409, '그대로': 1410, '그동안': 1411, '그만큼': 1412, '깨': 1413, '끼': 1414, '나름': 1415, '낚시질': 1416, '낼': 1417, '느낀': 1418, '단지': 1419, '단합': 1420, '담배': 1421, '뒤늦': 1422, '든다': 1423, '등록': 1424, '력': 1425, '로서': 1426, '마음속': 1427, '밀려오': 1428, '바람': 1429, '바보': 1430, '바지': 1431, '보성': 1432, '분노': 1433, '불면증': 1434, '살아온': 1435, '시킨': 1436, '식스': 1437, '심금': 1438, '아쉬운': 1439, '얼굴': 1440, '여태': 1441, '유산': 1442, '인셉션': 1443, '자극': 1444, '정상': 1445, '정의': 1446, '진출': 1447, '초딩': 1448, '초월': 1449, '칠': 1450, '케': 1451, '코': 1452, '키': 1453, '팬티': 1454, '합격': 1455, '휴지': 1456, '.?': 1457, '99': 1458, 'no': 1459, 't': 1460, 'x': 1461, 'ㅇㅇ': 1462, 'ㅠㅠㅠㅠ': 1463, '걸리': 1464, '나눌': 1465, '나와': 1466, '놓치': 1467, '는다는': 1468, '달리': 1469, '당연히': 1470, '당첨': 1471, '동시대': 1472, '드렸': 1473, '디스크': 1474, '레옹': 1475, '먹먹': 1476, '몰': 1477, '바꾸': 1478, '바꾼': 1479, '버금가': 1480, '병원': 1481, '보석': 1482, '비하': 1483, '빠져': 1484, '세계인': 1485, '시기': 1486, '시리즈': 1487, '쓸': 1488, '양심': 1489, '어린': 1490, '예요': 1491, '오줌': 1492, '외국': 1493, '외쳤': 1494, '이세상': 1495, '자면': 1496, '자존심': 1497, '정녕': 1498, '젖': 1499, '짧': 1500, '추': 1501, '터': 1502, '티비': 1503, '헛살': 1504, '14': 1505, '140': 1506, '^': 1507, 'best': 1508, '★': 1509, '가득': 1510, '가히': 1511, '각본': 1512, '거나': 1513, '겨우': 1514, '격투': 1515, '결말': 1516, '고자': 1517, '구매': 1518, '그럼': 1519, '글자': 1520, '김정은': 1521, '끈': 1522, '나와서': 1523, '놀라운': 1524, '눈물나': 1525, '느껴진다': 1526, '더라구요': 1527, '던가': 1528, '되게': 1529, '되돌아보': 1530, '땀': 1531, '롤': 1532, '륵': 1533, '만세': 1534, '망했': 1535, '명연기': 1536, '목적': 1537, '무언가': 1538, '밥': 1539, '벌': 1540, '벤허': 1541, '봐요': 1542, '부르': 1543, '비로소': 1544, '빵': 1545, '새롭': 1546, '생기': 1547, '선사': 1548, '술': 1549, '스러운': 1550, '스러울': 1551, '써': 1552, '언젠가': 1553, '연인': 1554, '예': 1555, '원래': 1556, '일생': 1557, '장애': 1558, '저리': 1559, '적극': 1560, '전환점': 1561, '접했': 1562, '제게': 1563, '존': 1564, '죽음': 1565, '짜증': 1566, '크리스토퍼': 1567, '하나하나': 1568, '해외': 1569, '힐링': 1570, '2000': 1571, 'you': 1572, '가린다': 1573, '간만에': 1574, '거기': 1575, '거대': 1576, '거장': 1577, '거짓': 1578, '걸까': 1579, '계기': 1580, '구만': 1581, '그게': 1582, '그야말로': 1583, '나마': 1584, '네여': 1585, '느냐': 1586, '단결력': 1587, '류': 1588, '리스트': 1589, '마르': 1590, '망설임': 1591, '멋지': 1592, '명성': 1593, '모른다': 1594, '모욕': 1595, '봤었': 1596, '부성애': 1597, '빛나': 1598, '사라지': 1599, '샘': 1600, '손가락': 1601, '수술': 1602, '식': 1603, '안다': 1604, '앞서': 1605, '어벤져스': 1606, '어쩔': 1607, '엄청나': 1608, '열연': 1609, '왠지': 1610, '외': 1611, '우울증': 1612, '운': 1613, '유치': 1614, '으시': 1615, '이라도': 1616, '이만': 1617, '임혁필': 1618, '전무후무': 1619, '중요': 1620, '쥐': 1621, '진작': 1622, '찡': 1623, '쳐': 1624, '출연진': 1625, '탄': 1626, '파': 1627, '판': 1628, '폰': 1629, '필름': 1630, '하고': 1631, '하느님': 1632, '한계': 1633, '할아버지': 1634, '할지': 1635, '헐': 1636, '혹시': 1637, '!!!!!': 1638, '52': 1639, '60': 1640, '<': 1641, '>': 1642, 'dc': 1643, 'ost': 1644, 'this': 1645, 'top': 1646, '가요': 1647, '걱정': 1648, '고전': 1649, '구하': 1650, '굳이': 1651, '금세기': 1652, '기념비': 1653, '꺼내': 1654, '꽤': 1655, '끊': 1656, '나올까': 1657, '나옵니다': 1658, '나중': 1659, '낸': 1660, '는구나': 1661, '늘': 1662, '답니다': 1663, '당': 1664, '당하': 1665, '대동단결': 1666, '돌아왔': 1667, '되찾': 1668, '드립': 1669, '랄까': 1670, '매': 1671, '매길': 1672, '멀': 1673, '명절': 1674, '묻히': 1675, '뭉클': 1676, '방법': 1677, '번방': 1678, '봅시다': 1679, '빠졌': 1680, '빠지': 1681, '새해': 1682, '섞': 1683, '수상': 1684, '시각': 1685, '시네마': 1686, '시력': 1687, '실화': 1688, '심정': 1689, '쏘': 1690, '아프': 1691, '어렵': 1692, '여전히': 1693, '영어': 1694, '영화인': 1695, '옛날': 1696, '온다': 1697, '왕': 1698, '왠만': 1699, '외장': 1700, '외침': 1701, '의사': 1702, '일깨워': 1703, '적당': 1704, '정': 1705, '져': 1706, '중반': 1707, '줘': 1708, '지나가': 1709, '지정': 1710, '지쳐': 1711, '진주': 1712, '참고': 1713, '찾아보': 1714, '캬': 1715, '트': 1716, '픈': 1717, '플': 1718, '허접': 1719, '형용': 1720, '화이팅': 1721, '확실': 1722, '회자': 1723, '훈훈': 1724, '훗날': 1725, '훨씬': 1726, '흐': 1727, '!\"': 1728, 'ㅏ': 1729, '감당': 1730, '걍': 1731, '경의': 1732, '고로': 1733, '구분': 1734, '그건': 1735, '기념': 1736, '긴장감': 1737, '까진': 1738, '깐': 1739, '남길': 1740, '노라': 1741, '누워': 1742, '느껴졌': 1743, '느낍니다': 1744, '담긴': 1745, '댓': 1746, '두고두고': 1747, '등재': 1748, '땅': 1749, '라스': 1750, '막히': 1751, '맘': 1752, '맛': 1753, '망한': 1754, '먼': 1755, '명장면': 1756, '명품': 1757, '몰라도': 1758, '무한': 1759, '문제': 1760, '반복': 1761, '빌려': 1762, '서로': 1763, '선수': 1764, '성경': 1765, '속지': 1766, '스스로': 1767, '실망': 1768, '썩': 1769, '쓴': 1770, '아닐': 1771, '아오': 1772, '안구건조증': 1773, '앉': 1774, '앉은뱅이': 1775, '앓': 1776, '압권': 1777, '영화계': 1778, '올려': 1779, '욕하': 1780, '으신': 1781, '이딴': 1782, '이때': 1783, '장담': 1784, '재생': 1785, '전쟁': 1786, '전체': 1787, '전화': 1788, '절대로': 1789, '절로': 1790, '정점': 1791, '정확히': 1792, '젠장': 1793, '졸라': 1794, '주먹': 1795, '준다는': 1796, '증명': 1797, '지친': 1798, '참회': 1799, '창조': 1800, '최소': 1801, '컨데': 1802, '킬링': 1803, '테': 1804, '평화': 1805, '풀': 1806, '필수': 1807, '하아': 1808, '한데': 1809, '........': 1810, '..?': 1811, '18': 1812, '2019': 1813, '???': 1814, '~!!!': 1815, '“': 1816, '가슴속': 1817, '거든요': 1818, '걷': 1819, '걸려서': 1820, '곱': 1821, '구요': 1822, '구입': 1823, '극복': 1824, '깎': 1825, '꺾': 1826, '끄': 1827, '끝내': 1828, '남겨': 1829, '느낀다': 1830, '다만': 1831, '닥치': 1832, '더불': 1833, '던지': 1834, '동네': 1835, '들리': 1836, '등장': 1837, '떠오르': 1838, '려면': 1839, '르': 1840, '마땅': 1841, '만날': 1842, '만남': 1843, '만났': 1844, '말기': 1845, '멋있': 1846, '명량': 1847, '못해': 1848, '무릎': 1849, '방송': 1850, '벅차': 1851, '보물': 1852, '부탁': 1853, '분위기': 1854, '불가': 1855, '비록': 1856, '살아왔': 1857, '상평': 1858, '샐러맨더': 1859, '생략': 1860, '생활': 1861, '소식': 1862, '순수': 1863, '쉽': 1864, '씨발': 1865, '아껴': 1866, '아래': 1867, '알려준': 1868, '약간': 1869, '어릴': 1870, '언어': 1871, '영감': 1872, '요소': 1873, '용서': 1874, '울음': 1875, '월드': 1876, '위함': 1877, '은가': 1878, '은서': 1879, '이끌': 1880, '일본': 1881, '입대': 1882, '전해': 1883, '조용히': 1884, '졸': 1885, '주위': 1886, '진부': 1887, '질문': 1888, '차마': 1889, '차이': 1890, '참여': 1891, '책': 1892, '카타르시스': 1893, '퀄리티': 1894, '통일': 1895, '평소': 1896, '포': 1897, '함부로': 1898, '항암': 1899, '허': 1900, '헝': 1901, '현': 1902, '혹시나': 1903, '후반': 1904, '후속작': 1905, '휴머니즘': 1906, '흑': 1907, '흑흑': 1908, '흘러도': 1909, '흘려': 1910, '-_-': 1911, '.!!': 1912, '2012': 1913, '70': 1914, '=': 1915, '”': 1916, '개월': 1917, '건데': 1918, '고마워요': 1919, '공포': 1920, '과언': 1921, '관': 1922, '국가': 1923, '귓가': 1924, '기도': 1925, '기쁨': 1926, '기저귀': 1927, '깨달음': 1928, '나뉠': 1929, '나란히': 1930, '널리': 1931, '누르': 1932, '늙': 1933, '다녀': 1934, '다리': 1935, '단결': 1936, '대세': 1937, '대해서': 1938, '도중': 1939, '된다고': 1940, '듭니다': 1941, '러닝': 1942, '로써': 1943, '만족': 1944, '망설이': 1945, '매력': 1946, '머릿속': 1947, '명언': 1948, '모': 1949, '무서운': 1950, '무인도': 1951, '민족': 1952, '바라보': 1953, '방영': 1954, '불치병': 1955, '블록버스터': 1956, '생명': 1957, '서른': 1958, '서울대': 1959, '슬퍼서': 1960, '시원': 1961, '신화': 1962, '실제로': 1963, '아끼': 1964, '안구': 1965, '알리': 1966, '압': 1967, '액': 1968, '양': 1969, '어른': 1970, '억울': 1971, '업': 1972, '엉': 1973, '연속': 1974, '연출력': 1975, '오스카': 1976, '우정': 1977, '운다고': 1978, '울컥': 1979, '워': 1980, '으니까': 1981, '으라면': 1982, '으로써': 1983, '으론': 1984, '은지': 1985, '이여': 1986, '이하': 1987, '인듯': 1988, '일찍': 1989, '자라': 1990, '잠들': 1991, '접한': 1992, '제임스': 1993, '조연': 1994, '주옥같': 1995, '줄거리': 1996, '지옥': 1997, '질질': 1998, '짐': 1999, '촬영': 2000, '충분히': 2001, '치카': 2002, '컨대': 2003, '케인': 2004, '터미네이터': 2005, '학년': 2006, '헛': 2007, '현대': 2008, '혹은': 2009, '확': 2010, '후손': 2011, '휴먼': 2012, '흠': 2013, '..!': 2014, '16': 2015, '2014': 2016, '26': 2017, '[': 2018, '`': 2019, 's': 2020, 'up': 2021, '~~!!': 2022, '가까운': 2023, '가려': 2024, '각': 2025, '간절히': 2026, '갈아입': 2027, '강': 2028, '강력히': 2029, '계획': 2030, '공유': 2031, '관계': 2032, '광광': 2033, '구원': 2034, '그걸': 2035, '그리': 2036, '깔': 2037, '꽃': 2038, '끼리': 2039, '나간': 2040, '나와야': 2041, '내려가': 2042, '너희': 2043, '넌': 2044, '넘치': 2045, '노력': 2046, '놀': 2047, '놀라': 2048, '누': 2049, '눈시울': 2050, '니까요': 2051, '다라마': 2052, '단순히': 2053, '대통령': 2054, '덜': 2055, '도장': 2056, '돌아오': 2057, '둘째': 2058, '등등': 2059, '땔': 2060, '떄': 2061, '뜨겁': 2062, '렷': 2063, '림': 2064, '마블': 2065, '매니아': 2066, '맥주': 2067, '메마른': 2068, '멜': 2069, '명배우': 2070, '모여': 2071, '무시': 2072, '밀려': 2073, '바치': 2074, '발차기': 2075, '버': 2076, '보소': 2077, '보지': 2078, '봣는데': 2079, '불멸': 2080, '븐': 2081, '블': 2082, '비슷': 2083, '빠': 2084, '빨': 2085, '사과나무': 2086, '산': 2087, '샀': 2088, '상당히': 2089, '생전': 2090, '숀': 2091, '시급': 2092, '신기': 2093, '싸움': 2094, '쌌': 2095, '썼': 2096, '안타까울': 2097, '압도': 2098, '영역': 2099, '영화상': 2100, '예상': 2101, '오지': 2102, '올라오': 2103, '왓': 2104, '외칠': 2105, '우양': 2106, '운동': 2107, '의한': 2108, '이곳': 2109, '이길': 2110, '이혼': 2111, '일부': 2112, '일부러': 2113, '일품': 2114, '입감': 2115, '자고': 2116, '작가': 2117, '젊': 2118, '종일': 2119, '주기': 2120, '주어진다면': 2121, '줄줄': 2122, '지리': 2123, '질리': 2124, '집니다': 2125, '쩔': 2126, '추진': 2127, '출근': 2128, '출시': 2129, '충무로': 2130, '취업': 2131, '치질': 2132, '카메론': 2133, '코미디': 2134, '코믹': 2135, '태극기': 2136, '털': 2137, '통틀어': 2138, '필히': 2139, '허리': 2140, '호기심': 2141, '호불호': 2142, '화질': 2143, '확신': 2144, '확실히': 2145, '후세': 2146, '!!!!!!!!!!': 2147, '!!!!!!!!!!!!!!!!!!!!!!!!!': 2148, '+': 2149, '.........': 2150, '.........................': 2151, '10000': 2152, '2013': 2153, '31': 2154, ']': 2155, 'imax': 2156, 'it': 2157, 'p': 2158, 'xx': 2159, 'ㅠㅠㅠㅠㅠ': 2160, '가까이': 2161, '가즈': 2162, '갈리': 2163, '감탄': 2164, '값': 2165, '강아지': 2166, '개꿀': 2167, '거리': 2168, '건강': 2169, '검프': 2170, '결정': 2171, '고야': 2172, '구라': 2173, '국': 2174, '그치': 2175, '기술': 2176, '기암': 2177, '기업': 2178, '기원': 2179, '꾸': 2180, '꿇': 2181, '나온다는': 2182, '내린': 2183, '너무너무': 2184, '넘어선': 2185, '노우': 2186, '논': 2187, '다세포': 2188, '당당히': 2189, '당연': 2190, '대답': 2191, '도전': 2192, '된다는': 2193, '두말': 2194, '들어도': 2195, '디시': 2196, '따로': 2197, '떠올리': 2198, '떡밥': 2199, '떨리': 2200, '뜰': 2201, '라인': 2202, '렸': 2203, '리플': 2204, '메세지': 2205, '모아': 2206, '모자라': 2207, '목표': 2208, '몰래': 2209, '무덤': 2210, '미리': 2211, '바다': 2212, '밤새': 2213, '방울': 2214, '버릴': 2215, '변하': 2216, '사투리': 2217, '상황': 2218, '서스펙트': 2219, '손발': 2220, '슬퍼': 2221, '식물': 2222, '쏟아지': 2223, '쓴다': 2224, '씻': 2225, '아쉬울': 2226, '아픈': 2227, '안본': 2228, '알람': 2229, '억지': 2230, '에게서': 2231, '열정': 2232, '오막살이': 2233, '완성도': 2234, '완전히': 2235, '외국인': 2236, '우린': 2237, '우왕': 2238, '울림': 2239, '울부짖': 2240, '위하': 2241, '으셨': 2242, '의해': 2243, '이래서': 2244, '인터넷': 2245, '잉': 2246, '자세': 2247, '잔': 2248, '잔잔': 2249, '잖아요': 2250, '장난질': 2251, '적절': 2252, '전문가': 2253, '조화': 2254, '주년': 2255, '주르륵': 2256, '중국': 2257, '증거': 2258, '지상': 2259, '지칠': 2260, '직장': 2261, '척': 2262, '첫째': 2263, '쵝오의': 2264, '축구': 2265, '카메라': 2266, '코로나': 2267, '쿼터': 2268, '탈': 2269, '판권': 2270, '펀드': 2271, '평균': 2272, '평론': 2273, '포레스트': 2274, '필': 2275, '한구석': 2276, '한국어': 2277, '해야지': 2278, '해질': 2279, '핵': 2280, '향해': 2281, '허무': 2282, '호평': 2283, '혼': 2284, '홀로': 2285, '효과': 2286, '흘': 2287, '흘러': 2288, '희생': 2289})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRL0-fWFwwSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#텍스트의 데이터로더 개념\n",
        "from torchtext.data import Iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN02YVn-wwQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\n",
        "test_loader = Iterator(dataset=test_data, batch_size = batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkCOzMZVwwNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "edacecb2-f204-4f92-9f31-4c9e5790a5a6"
      },
      "source": [
        "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
        "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 미니 배치 수 : 375\n",
            "테스트 데이터의 미니 배치 수 : 94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ2dHuetw1ss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "13a16054-dd16-418d-c5d3-31a7d768b1f4"
      },
      "source": [
        "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
        "print(batch.comment)\n",
        "print(batch.score)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 167,    5,  126,  ...,   78,    2,    2],\n",
            "        [ 851,    3,    0,  ...,    1,    1,    1],\n",
            "        [   0,  136,   57,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [  31,   30,   15,  ...,    1,    1,    1],\n",
            "        [ 256,   18,   38,  ...,    1,    1,    1],\n",
            "        [1039,  166,  183,  ...,    1,    1,    1]])\n",
            "tensor([10,  1,  9, 10, 10, 10, 10, 10,  1, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         1, 10, 10, 10, 10, 10, 10, 10,  1, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "        10, 10, 10,  1, 10,  1, 10,  1, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YtZyN9sAtuJ",
        "colab_type": "text"
      },
      "source": [
        "GPU사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqQkpjGSxKAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f82a076-03a0-487b-eada-5bd20a15c8a6"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54zijkCPAwGY",
        "colab_type": "text"
      },
      "source": [
        "모델생성 pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ls9lU4nw1oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "learning_rate = 0.05\n",
        "training_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYZ70D4Zw1m3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(nn.Linear(20, 32),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(32, 64),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(64, 128),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(128, 256),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(256,11) \n",
        "                                   )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-qqGGs6w1lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Classifier()\n",
        "criterion = torch.nn.CrossEntropyLoss()   \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGfrbD6w1iE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e934170-6251-4033-c72d-5189a127da61"
      },
      "source": [
        "total_batch = len(train_loader)\n",
        "print('총 배치의 수 : {}'.format(total_batch))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "총 배치의 수 : 375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVut5obeHlr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "304843f9-e454-478c-950a-ec10f42f79c0"
      },
      "source": [
        "next(iter(train_loader))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 64]\n",
              "\t[.comment]:[torch.LongTensor of size 64x20]\n",
              "\t[.score]:[torch.LongTensor of size 64]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtP8w6mAw7bS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5740ffec-4c97-4ca1-e173-4d35c665d84b"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in train_loader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y는 레이블.\n",
        "        # image is already size of (28x28), no reshape\n",
        "        # label is not one-hot encoded\n",
        "        X = X.to(torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        print(cost)\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7.9169, grad_fn=<NllLossBackward>)\n",
            "tensor(286.9445, grad_fn=<NllLossBackward>)\n",
            "tensor(12.7065, grad_fn=<NllLossBackward>)\n",
            "tensor(3.8122, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2897, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9712, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2307, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9747, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2536, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2633, grad_fn=<NllLossBackward>)\n",
            "tensor(5.5315, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0993, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0895, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8370, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2430, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6126, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5709, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0883, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4766, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9993, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6503, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8153, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3415, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7422, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4511, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6603, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4318, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6231, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6169, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5958, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6749, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6276, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7201, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8432, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5174, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5768, grad_fn=<NllLossBackward>)\n",
            "tensor(24.3059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5463, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4797, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2901, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5213, grad_fn=<NllLossBackward>)\n",
            "tensor(1.2148, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6919, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4693, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4091, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4673, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5929, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7159, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7734, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5493, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7280, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6790, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4275, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4985, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2820, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6260, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4534, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7515, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7532, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7428, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4699, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5403, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5318, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6534, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4669, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8310, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7135, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7513, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7206, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3698, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4356, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7922, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5620, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4459, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7103, grad_fn=<NllLossBackward>)\n",
            "tensor(19.8928, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7062, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6595, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7355, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6225, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8491, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6675, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6795, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8251, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6522, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7512, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4451, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5339, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6501, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6470, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3907, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0441, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6136, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4824, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6696, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9091, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4864, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5712, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6416, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4994, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6579, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7132, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6406, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7709, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5930, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5756, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6621, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6487, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5904, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6171, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5136, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5047, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5277, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4520, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5855, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5101, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5516, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7894, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5649, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5445, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5060, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5363, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4891, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6366, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8045, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7115, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4342, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5098, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5861, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5220, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5044, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7636, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3925, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5546, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6324, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5982, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4339, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4929, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3711, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3996, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6506, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7818, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5133, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4800, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5656, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4539, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3116, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7364, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4772, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7321, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5604, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4506, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5316, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5321, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5330, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3195, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5085, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6091, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4719, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5750, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5677, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6332, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4508, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7240, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4219, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4752, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7483, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6657, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4593, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5351, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7101, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6505, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5723, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8047, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4929, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4053, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6922, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4724, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5513, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5342, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5048, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5051, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5539, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5996, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5771, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8379, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5687, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6132, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6230, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5777, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8596, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5414, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4404, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4278, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6051, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    1] cost = 1.56183231\n",
            "tensor(0.3814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5344, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5392, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7501, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6007, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8575, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4963, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6192, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4928, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5600, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6256, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4958, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7935, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5269, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4784, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7897, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4612, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3834, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4739, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5382, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7971, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5430, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4185, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6831, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5304, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5148, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6444, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5503, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7220, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4828, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6770, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4512, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8533, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6581, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6711, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8455, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6513, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5842, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6683, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6445, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5830, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5713, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8686, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8607, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4062, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4053, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7944, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6052, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4604, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5715, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4388, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4509, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5418, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4682, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4331, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4971, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8528, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5467, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5468, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5208, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6810, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4848, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4698, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5141, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6200, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0015, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4380, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4374, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3933, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6487, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4238, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5891, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6478, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5712, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7732, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3670, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5058, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4299, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6028, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6730, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6255, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6492, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5719, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7702, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5144, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6587, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6328, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6062, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4433, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5244, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5362, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5682, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5414, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5304, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5767, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6259, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7060, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5346, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4357, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5212, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6432, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6304, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5259, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6277, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6119, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3817, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7110, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5510, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5520, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5133, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5987, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5252, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8192, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5575, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5653, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6239, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5119, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4659, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6608, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7260, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5315, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5561, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5817, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7428, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6364, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8879, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6513, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5478, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7202, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4177, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5468, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4649, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5074, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6921, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4468, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7118, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6385, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5045, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6155, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7279, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6559, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8088, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5439, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4682, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7159, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4828, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6131, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4866, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5030, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3756, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6636, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5683, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7197, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6221, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4830, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4303, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3628, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5605, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6277, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5291, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6837, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5176, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5162, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5924, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7241, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4422, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5808, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6987, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5157, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5094, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7927, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3147, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5457, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4739, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5577, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7600, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4505, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4641, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5975, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7055, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7444, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7208, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4694, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3535, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6603, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8099, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5592, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3645, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5163, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    2] cost = 0.582583487\n",
            "tensor(0.6753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8627, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6532, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5484, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6469, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5242, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6928, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6300, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4414, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5268, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4771, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6225, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4852, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5353, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5650, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5935, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7930, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5048, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5598, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6552, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5880, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5747, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4743, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7083, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5080, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5094, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7639, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5448, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6433, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6277, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4480, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5432, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4621, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6173, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4241, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5500, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4503, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5837, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6994, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6177, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5088, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5141, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4826, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4598, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5856, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6313, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6433, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4559, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5176, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6299, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5524, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6127, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6388, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7100, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6028, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4374, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5436, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4873, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7756, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4897, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4874, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5176, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5166, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4637, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6388, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4901, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8955, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4162, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7291, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3468, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5845, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6285, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5946, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5629, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7500, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4904, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5494, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5051, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4303, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5221, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6357, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6355, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4283, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7173, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4778, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4999, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5268, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5540, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5096, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6088, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5041, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5279, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7377, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5780, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4894, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5780, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5145, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4504, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4711, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3147, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5705, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4578, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5467, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8968, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8079, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4298, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7493, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5380, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4376, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6628, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7531, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6518, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6900, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5670, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6320, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5201, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5204, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5676, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4998, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8780, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5529, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6376, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5460, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6650, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6802, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6236, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5736, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5434, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8176, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6895, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4894, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4842, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7191, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5638, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4484, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7891, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9482, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6094, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4367, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6639, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4603, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5091, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4702, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7307, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5060, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4278, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5094, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4819, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5179, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7906, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6012, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6631, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4434, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6570, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9546, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8047, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4414, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6112, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6416, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6855, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6894, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4465, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8132, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5054, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5058, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7100, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4866, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6063, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5398, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7000, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5610, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6269, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4515, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6816, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6429, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5341, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6958, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6444, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5323, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    3] cost = 0.582617104\n",
            "tensor(0.5439, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5241, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5517, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7348, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5541, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6694, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4356, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4374, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6324, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6483, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4924, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4074, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6104, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4698, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5141, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6030, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6098, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5430, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5060, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4201, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5051, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4606, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5482, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4272, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5958, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3666, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6085, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3029, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0117, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7405, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8830, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5210, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5110, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5511, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4520, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5120, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9194, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4456, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3198, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4331, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9734, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6444, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6461, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3110, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6642, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5500, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5100, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5848, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7367, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6831, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5843, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8366, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4988, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6320, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4614, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6377, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5441, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4485, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6569, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4351, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5775, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5501, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7577, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7071, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5659, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7603, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5116, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5077, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6216, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4448, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5938, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4982, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5152, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5376, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7089, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4674, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5253, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4805, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4466, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7505, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3834, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3674, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6329, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5249, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4509, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4239, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5977, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6052, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5575, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5110, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5297, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5622, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5568, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5565, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4300, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6747, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7493, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5782, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8128, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4424, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4119, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6749, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6372, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4596, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8198, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7581, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6237, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3706, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4386, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6083, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9166, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4612, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7528, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4578, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4052, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7084, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7362, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6263, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6985, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5303, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4668, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5011, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6652, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3283, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5768, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6071, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5594, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5833, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4951, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8397, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6543, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6226, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6928, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7895, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5236, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4621, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5280, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4138, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8157, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7174, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5320, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6392, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5975, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7637, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5690, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5417, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4472, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6484, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6279, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4329, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5524, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5136, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7494, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7560, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6809, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5563, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5223, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3180, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3556, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7048, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0240, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8650, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5475, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5540, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6984, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6221, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7405, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7179, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5303, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7778, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4428, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5543, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6138, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4535, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5709, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4608, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7392, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5929, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7434, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6639, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3383, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6302, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6066, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4438, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5181, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5374, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6136, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    4] cost = 0.580714464\n",
            "tensor(0.5576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6404, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5276, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6314, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6169, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5055, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4216, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7071, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6104, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4484, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5274, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5414, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5311, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5196, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5039, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5219, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5620, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5732, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6995, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5418, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4441, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7879, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6475, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5622, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5491, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7285, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6047, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4462, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6486, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7598, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4636, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5522, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6999, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5408, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7431, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5458, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5039, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5706, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4606, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5291, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5784, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4374, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7653, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6157, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5817, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6285, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6775, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6366, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5820, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6400, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5252, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4157, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2668, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7314, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4223, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7789, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4586, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6478, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5893, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5055, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6482, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3484, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6080, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6977, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5397, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6221, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5238, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4832, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8231, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4573, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5563, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5552, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5366, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3913, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6984, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4475, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5656, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5556, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6013, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5655, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5481, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4239, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5246, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7372, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5303, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4579, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6460, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7155, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4652, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6174, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8031, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6864, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8124, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6677, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4782, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5045, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6311, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3596, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5046, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6041, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5256, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6300, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6201, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4989, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5878, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3594, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5161, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5704, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5366, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5232, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6329, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5219, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8223, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4767, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4650, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6032, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4836, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6363, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4587, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5299, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5400, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4843, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6243, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4696, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9236, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6113, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6413, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7524, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7162, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6420, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5501, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6523, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6842, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8397, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4559, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5164, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6272, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5468, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4920, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7452, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5436, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5577, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6072, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4432, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5564, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5631, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6817, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5823, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5468, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4401, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5194, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6563, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6030, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6797, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6616, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7382, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7594, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4750, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4302, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5260, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4807, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3856, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4285, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8590, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8445, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6154, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5569, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5951, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5942, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5220, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5580, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7797, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5271, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    5] cost = 0.580100596\n",
            "tensor(0.7627, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5474, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4451, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5683, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6350, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6382, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4561, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5062, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4784, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5256, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7368, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5780, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5745, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4517, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7357, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5972, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5246, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5266, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5260, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5561, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5412, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4013, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4782, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5323, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5832, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5226, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6276, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5332, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3480, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4928, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5504, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6938, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7681, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5362, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5794, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5216, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6968, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4594, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5524, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6403, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8052, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5417, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6785, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8437, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5136, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8192, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7409, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6787, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8404, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6413, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6425, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4895, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7923, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5404, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3921, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5998, grad_fn=<NllLossBackward>)\n",
            "tensor(1.0059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5531, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5609, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7209, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5596, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5787, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5335, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4007, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5692, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6784, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5510, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3990, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5783, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5954, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6350, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4216, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6475, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5987, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4192, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6112, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5409, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6376, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3207, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5039, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5626, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7854, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6077, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6679, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4041, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4866, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6461, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4198, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3478, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5978, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4569, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6497, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8775, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5927, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5873, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5694, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5110, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6844, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5457, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4683, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6502, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6020, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5255, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6428, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5626, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9676, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3857, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5131, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6995, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5656, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8510, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5447, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5119, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6025, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5372, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5690, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8194, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5730, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6501, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3383, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6159, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6125, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5203, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5325, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4417, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5818, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5284, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6491, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5690, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5310, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5188, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5004, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5916, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3353, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6051, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7284, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6791, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4629, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5471, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3849, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5535, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5590, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5275, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5951, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4417, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6126, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7955, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7375, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7795, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5643, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6284, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7756, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5605, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8054, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5331, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6921, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5242, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7890, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4592, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4127, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4998, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7958, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5657, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6210, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3237, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5355, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5327, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6189, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4084, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6389, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7112, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4993, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3577, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7388, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4286, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6733, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7807, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5807, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5505, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5431, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4944, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5116, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6068, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4692, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3969, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5785, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7732, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8371, grad_fn=<NllLossBackward>)\n",
            "tensor(1.1516, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4627, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6011, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6110, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6720, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6457, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5790, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9226, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5246, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3840, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6669, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6179, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4489, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6719, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3497, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6638, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    6] cost = 0.579432368\n",
            "tensor(0.7308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5818, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4300, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6203, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5233, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7041, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5748, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6671, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4751, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6883, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5657, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4706, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2990, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3963, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7963, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5639, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6891, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6334, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5705, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7004, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5459, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7133, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7453, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5507, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4416, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4176, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4561, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8195, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6535, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5389, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6251, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6683, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7424, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2977, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6255, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5011, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7384, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5534, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7990, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5856, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5760, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6600, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7192, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4564, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4114, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4605, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5244, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4743, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4090, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5593, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5903, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6273, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6459, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8140, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6493, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8077, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3891, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4872, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6950, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4463, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3925, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6011, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8897, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7528, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5083, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4795, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5098, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6712, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3994, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6482, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8374, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4631, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6357, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7304, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3304, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5635, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5762, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6294, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7272, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4325, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5918, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4426, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3071, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5162, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7150, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5968, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6197, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5159, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6441, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8075, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4386, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6194, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5561, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3335, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5297, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4672, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4612, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7165, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6117, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5416, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6346, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7481, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4478, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5302, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6298, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6480, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6620, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3549, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4864, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7071, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8411, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3856, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5520, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5343, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4772, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4351, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9015, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6406, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6451, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5529, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4767, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7206, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5505, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6878, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5754, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5938, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6465, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4376, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4068, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5263, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5736, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7324, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4144, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4259, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5142, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6644, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7989, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5822, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5922, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4711, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8013, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4239, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3483, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5598, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6454, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6378, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7135, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4474, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4624, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6734, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7509, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5655, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6880, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5460, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7864, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5662, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7868, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5260, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5136, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3437, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5593, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4986, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4700, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5385, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6154, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4047, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6641, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4797, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3798, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6507, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5647, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6197, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5642, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5266, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6894, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4852, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5738, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7709, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6779, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7138, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7066, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6310, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4367, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6434, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5666, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4057, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5860, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5701, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5001, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6259, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    7] cost = 0.580186069\n",
            "tensor(0.4882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6346, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4634, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6482, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4424, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3366, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3965, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6963, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5935, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4251, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5964, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6187, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6974, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4728, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5946, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6855, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4100, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5270, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6931, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8393, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6508, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6216, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5461, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4437, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4439, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5619, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4238, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5804, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5972, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6578, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7984, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7092, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5444, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7821, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6601, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6240, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7135, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6175, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5932, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6974, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6037, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6218, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3313, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6687, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5520, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7810, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7409, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5080, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5699, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3566, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5691, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6774, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6854, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5100, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6487, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5856, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5153, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5580, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7410, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4330, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5315, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6512, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6626, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6055, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4067, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6870, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5669, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6183, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5610, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4606, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6426, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3812, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7379, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8121, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5435, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6197, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8203, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7436, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6523, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4363, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5082, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5306, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6885, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7502, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7283, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5924, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4673, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4680, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6395, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7076, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7656, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6022, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8026, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5591, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6451, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4988, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6505, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7852, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5107, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5649, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5553, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6242, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4415, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3460, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5457, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5078, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4127, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5139, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5740, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4558, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4964, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4693, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6787, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6966, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5328, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6069, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5503, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7541, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6563, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6186, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5787, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6973, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6162, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4342, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4603, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5693, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5132, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6106, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6202, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3724, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5845, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4261, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4528, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5753, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5577, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3074, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6202, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9470, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7327, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5490, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3249, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6226, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3692, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4790, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6871, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6414, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4437, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5624, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6504, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3995, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6451, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4926, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6347, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4501, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5231, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6480, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5745, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7132, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8201, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5847, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5634, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4294, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4003, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5842, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5412, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4803, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5074, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7352, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4367, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8360, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3999, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3721, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5848, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6003, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5172, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5451, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4183, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3493, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4622, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4545, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5543, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4752, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4705, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5406, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3160, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3427, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5845, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7428, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7790, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6081, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6397, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7269, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6048, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4608, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5322, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4845, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6900, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4386, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4087, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6580, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8315, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6600, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5848, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7690, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4886, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4041, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7104, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5048, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7166, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7838, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3911, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4961, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4192, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8864, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5195, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6279, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4998, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5391, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7497, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5630, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5741, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4818, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8478, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3552, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4074, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5646, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5630, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    8] cost = 0.578639805\n",
            "tensor(0.6845, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4438, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6519, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5234, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6311, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4193, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6636, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8072, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6352, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4964, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5012, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7827, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7977, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5593, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3345, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5898, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4256, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5190, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6051, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4916, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4034, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6651, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5054, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7887, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6220, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5937, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6145, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6697, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5014, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2642, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6873, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6976, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6571, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4328, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5541, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5422, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4320, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4058, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3788, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4279, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5157, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5935, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4232, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4496, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6066, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6610, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3631, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7341, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4637, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6594, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6455, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3564, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6058, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6684, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7981, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7280, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4818, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5611, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4659, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6290, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5910, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5113, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3426, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6194, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7945, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6985, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4345, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7177, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5167, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7914, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5771, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6835, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6882, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7249, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6132, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5289, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6475, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7778, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6758, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7098, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4460, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6849, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5861, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5182, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5723, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5108, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5099, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5546, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6901, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4142, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6768, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7859, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4861, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6947, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4250, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4708, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7979, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6394, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6662, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6431, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5629, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6495, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3412, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6537, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7500, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5703, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6139, grad_fn=<NllLossBackward>)\n",
            "tensor(0.2853, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4506, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4353, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5889, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5685, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4985, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4636, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7057, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5624, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5206, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7154, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6858, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4728, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5524, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5732, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5285, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6153, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6113, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4382, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8732, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8185, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4665, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5429, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6742, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4917, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3643, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4387, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5295, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5556, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6364, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5670, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3750, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4439, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4851, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4248, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5710, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5118, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4531, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7617, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6952, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8157, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7605, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8195, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4517, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4769, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5044, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7094, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6423, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6428, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5240, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5962, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5266, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5275, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3551, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4830, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4293, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5189, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8143, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4865, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3554, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6946, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6386, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7104, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5729, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5404, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5246, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4983, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6276, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6616, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6215, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5658, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5717, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5919, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5513, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5749, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6918, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4091, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5763, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6174, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6442, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5253, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5930, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4589, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5390, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5245, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6900, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7023, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5640, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6934, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4824, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5258, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3725, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4219, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5731, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4456, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6487, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8355, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7364, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6170, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8024, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5854, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7419, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4830, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4584, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6999, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4099, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6111, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6247, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5666, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6028, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5344, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8057, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6415, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5375, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6578, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5291, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5846, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4380, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6446, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5346, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5525, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5210, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5005, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4476, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6061, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5212, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4527, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5426, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4507, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6178, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5707, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4783, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5904, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5907, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3943, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4413, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5029, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5488, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4954, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3196, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6941, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9339, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6443, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5441, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4275, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5460, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9781, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9018, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6171, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4666, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4544, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6206, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7130, grad_fn=<NllLossBackward>)\n",
            "[Epoch:    9] cost = 0.578491926\n",
            "tensor(0.4498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7757, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5744, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6877, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6637, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5181, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5555, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7881, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6179, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6123, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7358, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4977, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5341, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6086, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6675, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4432, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4694, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5465, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5813, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6230, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6088, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3597, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6621, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4916, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4911, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6368, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6154, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7120, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6759, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5986, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5780, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5915, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3902, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4625, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5498, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6492, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4365, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5042, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4654, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6137, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4206, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5633, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6967, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6286, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5377, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5396, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6502, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4821, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4296, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9796, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5695, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5531, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6095, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5335, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4021, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5061, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5456, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5765, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7550, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5308, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7336, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5795, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6103, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4064, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4786, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5607, grad_fn=<NllLossBackward>)\n",
            "tensor(0.9239, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4281, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5337, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4988, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5122, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3998, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7217, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4724, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4252, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6514, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5421, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5655, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6214, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3980, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5547, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8078, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5875, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4542, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5228, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7845, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6548, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5282, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4992, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5292, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5197, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3811, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6117, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5223, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3714, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5618, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6663, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7777, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6398, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8089, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4009, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7002, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6359, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5477, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4483, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4043, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6151, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6431, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4503, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5588, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7618, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5538, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7035, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4920, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6572, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6939, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4578, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7073, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8131, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4953, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4528, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5065, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6500, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4577, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6173, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5033, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7349, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4321, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6461, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7814, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4737, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4402, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5019, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6908, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5287, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5185, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8104, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6755, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5163, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4876, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5581, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5669, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5602, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6899, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5678, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6211, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7437, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6017, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5948, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5455, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6355, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6623, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5309, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7567, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5177, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5576, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7450, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5481, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5662, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4648, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6726, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6265, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4773, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6650, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4652, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7453, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7369, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6254, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5565, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6670, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5583, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5440, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5493, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5404, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3936, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4660, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5059, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4530, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5718, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5196, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5499, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3674, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4361, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8116, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4587, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4310, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5674, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3778, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5839, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6286, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6766, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5592, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4930, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4320, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6892, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6188, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6036, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5317, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3735, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5371, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5158, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5422, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6321, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4305, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8449, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6991, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6399, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4115, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6792, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6010, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5200, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5050, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5667, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3797, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6560, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6222, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6093, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7926, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8165, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4909, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4825, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5301, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5668, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4829, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5764, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4003, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6331, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7271, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7257, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5056, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4032, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4256, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6118, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5610, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5084, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7168, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5070, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5574, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5820, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5315, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5288, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4552, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5761, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6867, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5810, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7776, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5793, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4312, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3959, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5326, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5325, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5268, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7613, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6148, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3836, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6661, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7354, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8240, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6040, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5183, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4109, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6146, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4267, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7831, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5381, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3562, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4971, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6949, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6475, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5145, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6918, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7526, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6027, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8664, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4815, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5235, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5595, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5205, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5334, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6940, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3536, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6970, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4850, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4100, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6806, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8096, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4615, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6662, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5956, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4888, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5520, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6319, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6481, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7049, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6485, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6342, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6599, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3746, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4801, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7007, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5264, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5037, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6716, grad_fn=<NllLossBackward>)\n",
            "tensor(0.3722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7600, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5722, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4841, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7632, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6202, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5873, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7473, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5997, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8512, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5008, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4581, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6105, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6149, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7389, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4580, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5134, grad_fn=<NllLossBackward>)\n",
            "tensor(0.8884, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5920, grad_fn=<NllLossBackward>)\n",
            "tensor(0.6911, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5007, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5098, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5912, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5689, grad_fn=<NllLossBackward>)\n",
            "tensor(0.5016, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4869, grad_fn=<NllLossBackward>)\n",
            "tensor(0.7590, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4229, grad_fn=<NllLossBackward>)\n",
            "tensor(0.4221, grad_fn=<NllLossBackward>)\n",
            "[Epoch:   10] cost = 0.578076422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1HWKRac-1hI",
        "colab_type": "text"
      },
      "source": [
        "쇼생크탈출 대본 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz4K8rubw7We",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "05fefe80-da50-4e93-d5e7-ad03aff05453"
      },
      "source": [
        "!pip install wget\n",
        "import wget"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=f1e92c538bd72cc5cd1b59ff100fc1548165c7442678a535079714ff4365eb85\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWf6Ju4m-8i-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "64cd628c-f38c-4634-9eff-61df9313dde2"
      },
      "source": [
        "#wget으로 내 깃허브에 올린 쇼생크탈출 url을 통해 다운로드\n",
        "#imdb에서 가장 평점이 높은 쇼생크탈출의 대본\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/leehyunggeunkeun/pytorch-study/master/%EC%87%BC%EC%83%9D%ED%81%AC%ED%83%88%EC%B6%9C.txt'"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-01 04:36:11--  https://raw.githubusercontent.com/leehyunggeunkeun/pytorch-study/master/%EC%87%BC%EC%83%9D%ED%81%AC%ED%83%88%EC%B6%9C.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180784 (177K) [text/plain]\n",
            "Saving to: ‘쇼생크탈출.txt’\n",
            "\n",
            "\r쇼생크탈출.txt        0%[                    ]       0  --.-KB/s               \r쇼생크탈출.txt      100%[===================>] 176.55K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-06-01 04:36:12 (6.77 MB/s) - ‘쇼생크탈출.txt’ saved [180784/180784]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTJrOpuD-8go",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "9e691cbe-b978-476f-df7e-39b2a4ca6060"
      },
      "source": [
        "input_file = open('쇼생크탈출.txt','r')\n",
        "lines = input_file.readlines()\n",
        "lines[:10]"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['     THE SHAWSHANK REDEMPTION\\n',\n",
              " '\\n',\n",
              " '                                 by\\n',\n",
              " '\\n',\n",
              " '                           Frank Darabont\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '                        Based upon the story \\n',\n",
              " '                Rita Hayworth and Shawshank Redemption\\n',\n",
              " '\\t                   by Stephen King\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSzUx1QD-8ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "temp_sentence = ''\n",
        "raw_sentence = ''\n",
        "\n",
        "#정규표현식으로 영어에 맞게 제거 \n",
        "\n",
        "for line in lines:\n",
        "    line = re.sub('[^a-zA-Z0-9.]',' ',line.lower())\n",
        "    raw_sentence+=line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg7yNVM8-8dY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "bdaefb3d-b2bf-4d84-a058-2beaeb4293b2"
      },
      "source": [
        "splited_sentence = raw_sentence.split('.')\n",
        "splited_sentence[:10]"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['     the shawshank redemption                                   by                             frank darabont                           based upon the story                  rita hayworth and shawshank redemption                     by stephen king    1 int    cabin    night  1946     a dark  empty room',\n",
              " '    the door bursts open',\n",
              " ' a man and woman enter  drunk and   giggling  horny as hell',\n",
              " ' no sooner is the door shut than   they re all over each other  ripping at clothes  pawing at   flesh  mouths locked together',\n",
              " '    he gropes for a lamp  tries to turn it on  knocks it over   instead',\n",
              " ' hell with it',\n",
              " ' he s got more urgent things to do  like   getting her blouse open and his hands on her breasts',\n",
              " ' she   arches  moaning  fumbling with his fly',\n",
              " ' he slams her against   the wall  ripping her skirt',\n",
              " ' we hear fabric tear']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RMSTLja-8ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleansed_splited_sentence = []\n",
        "\n",
        "for sentence in splited_sentence:\n",
        "    temp_sentence = ''\n",
        "    words = sentence.split(' ')\n",
        "    for word in words:\n",
        "        if word !='':\n",
        "            temp_sentence+=word+' '\n",
        "    if temp_sentence[:-1] == '':continue\n",
        "    cleansed_splited_sentence.append(temp_sentence[:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPC8PMICAV69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "3e3e7fa6-f8e0-4456-9307-9167cd7447ab"
      },
      "source": [
        "cleansed_splited_sentence[:10]"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the shawshank redemption by frank darabont based upon the story rita hayworth and shawshank redemption by stephen king 1 int cabin night 1946 a dark empty room',\n",
              " 'the door bursts open',\n",
              " 'a man and woman enter drunk and giggling horny as hell',\n",
              " 'no sooner is the door shut than they re all over each other ripping at clothes pawing at flesh mouths locked together',\n",
              " 'he gropes for a lamp tries to turn it on knocks it over instead',\n",
              " 'hell with it',\n",
              " 'he s got more urgent things to do like getting her blouse open and his hands on her breasts',\n",
              " 'she arches moaning fumbling with his fly',\n",
              " 'he slams her against the wall ripping her skirt',\n",
              " 'we hear fabric tear']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crbWsXujAV5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "9bac2987-ded8-496c-dd3f-f519f77632f4"
      },
      "source": [
        "splited_word = []\n",
        "for sentence in cleansed_splited_sentence:\n",
        "    words = sentence.split(' ')\n",
        "    splited_word+=words\n",
        "            \n",
        "splited_word[:10]"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'shawshank',\n",
              " 'redemption',\n",
              " 'by',\n",
              " 'frank',\n",
              " 'darabont',\n",
              " 'based',\n",
              " 'upon',\n",
              " 'the',\n",
              " 'story']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdoN6c2lAV2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c787d5b0-9ecf-4b38-ef3f-2477fc483197"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "stopwords.words('english')[:10]"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw20GUX1AZle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = []\n",
        "for word in splited_word:\n",
        "    if word not in stop_words:\n",
        "        result.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_JapRmMAZj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "7a5bc9ef-5014-4551-a80a-51477ffeb51c"
      },
      "source": [
        "result[:10]"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['shawshank',\n",
              " 'redemption',\n",
              " 'frank',\n",
              " 'darabont',\n",
              " 'based',\n",
              " 'upon',\n",
              " 'story',\n",
              " 'rita',\n",
              " 'hayworth',\n",
              " 'shawshank']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw76QM1eAZic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5574ac30-9e5b-4661-ed63-1e6ed404d48a"
      },
      "source": [
        "vocab = FreqDist(np.hstack(result))\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 4392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASeeLUi7AZdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b7f8910-34ee-4a58-a117-1d11a40639c4"
      },
      "source": [
        "vocab['surprise'] #surprise는 4개만 나옴 "
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9mVh8c8AebM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34662adf-9fc8-4aa3-fa43-ce86ae17107e"
      },
      "source": [
        "vocab_size = 1000\n",
        "# 상위 vocab_size개의 단어만 보존\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8pBZ4xKAeZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
        "word_to_index['pad'] = 1\n",
        "word_to_index['unk'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfKk9-DqAeVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded = []\n",
        "for line in result: #입력 데이터에서 1줄씩 문장을 읽음\n",
        "    temp = []\n",
        "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
        "      try:\n",
        "        temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
        "      except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
        "        temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
        "\n",
        "    encoded.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I0vKeuJAeTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ecfc6ec4-2d69-4882-c47c-19efd11a82de"
      },
      "source": [
        "print(encoded[:10])"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 250, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 250, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvANQhsdMqXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = max(len(l) for l in encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usNuz-WWMsB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d7f25cc6-3547-43b9-d379-d396365277cb"
      },
      "source": [
        "print('대본의 최대 길이 : %d' % max_len)\n",
        "print('대본의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
        "print('대본의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "대본의 최대 길이 : 17\n",
            "대본의 최소 길이 : 1\n",
            "대본의 평균 길이 : 5.084304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tORGmiyFMvCe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "39a22d7d-b1f0-4c63-f71a-a63beb0a62ff"
      },
      "source": [
        "plt.hist([len(s) for s in encoded], bins=20)\n",
        "plt.xlabel('length of sample')\n",
        "plt.ylabel('number of sample')\n",
        "plt.show()"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPEmBFJwNIUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in encoded:\n",
        "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
        "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzWvLh-pNQAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "de22a2ac-6160-47bd-93e9-907ccf2a5f06"
      },
      "source": [
        "print('리뷰의 최대 길이 : %d' % max(len(l) for l in encoded))\n",
        "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
        "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰의 최대 길이 : 17\n",
            "리뷰의 최소 길이 : 17\n",
            "리뷰의 평균 길이 : 17.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwsL3fuNNRNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "954d032f-89db-4cbf-8b4b-8f886bb09be0"
      },
      "source": [
        "print(encoded[0])"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}